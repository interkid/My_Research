{
  
    
        "post0": {
            "title": "Title",
            "content": "Reccomend system introduction . Goal . from interset users show, making methods for recommendation that user would have interset. („É¶„Éº„Ç∂„Éº„ÅåÁ§∫„Åó„Å¶„ÅÑ„ÇãÁèæÂú®„ÅÆËààÂë≥„Åã„Çâ„ÄÅÊñ∞„Åó„ÅèËààÂë≥„ÇíÊåÅ„Å§„Åß„ÅÇ„Çç„ÅÜÂàÜÈáé„Çí„ÅäÂãß„ÇÅ„Åô„ÇãÊñπÊ≥ï„Çí‰Ωú„Çã) . methods . reccomend ranking deceinding oreder (this aritcle don&#39;t treat) | user based collaborative filtering(„É¶„Éº„Ç∂„Éº„Éô„Éº„ÇπÂçîË™ø„Éï„Ç£„É´„Çø„É™„É≥„Ç∞) | item based collaborative filtering(„Ç¢„Ç§„ÉÜ„É†„Éô„Éº„ÇπÂçîË™ø„Éï„Ç£„É´„Çø„É™„É≥„Ç∞) | . . user based collaborative filtering . policy . search user who is silimilar to target user | suggest interest that group of users has to target user | . how to . use cosaine similarity (0: not same interst / 1 same interest ) | . users Data . users_interests = [ [&quot;Hadoop&quot;, &quot;Big Data&quot;, &quot;HBase&quot;, &quot;Java&quot;, &quot;Spark&quot;, &quot;Storm&quot;, &quot;Cassandra&quot;], [&quot;NoSQL&quot;, &quot;MongoDB&quot;, &quot;Cassandra&quot;, &quot;HBase&quot;, &quot;Postgres&quot;], [&quot;Python&quot;, &quot;scikit-learn&quot;, &quot;scipy&quot;, &quot;numpy&quot;, &quot;statsmodels&quot;, &quot;pandas&quot;], [&quot;R&quot;, &quot;Python&quot;, &quot;statistics&quot;, &quot;regression&quot;, &quot;probability&quot;], [&quot;machine learning&quot;, &quot;regression&quot;, &quot;decision trees&quot;, &quot;libsvm&quot;], [&quot;Python&quot;, &quot;R&quot;, &quot;Java&quot;, &quot;C++&quot;, &quot;Haskell&quot;, &quot;programming languages&quot;], [&quot;statistics&quot;, &quot;probability&quot;, &quot;mathematics&quot;, &quot;theory&quot;], [&quot;machine learning&quot;, &quot;scikit-learn&quot;, &quot;Mahout&quot;, &quot;neural networks&quot;], [&quot;neural networks&quot;, &quot;deep learning&quot;, &quot;Big Data&quot;, &quot;artificial intelligence&quot;], [&quot;Hadoop&quot;, &quot;Java&quot;, &quot;MapReduce&quot;, &quot;Big Data&quot;], [&quot;statistics&quot;, &quot;R&quot;, &quot;statsmodels&quot;], [&quot;C++&quot;, &quot;deep learning&quot;, &quot;artificial intelligence&quot;, &quot;probability&quot;], [&quot;pandas&quot;, &quot;R&quot;, &quot;Python&quot;], [&quot;databases&quot;, &quot;HBase&quot;, &quot;Postgres&quot;, &quot;MySQL&quot;, &quot;MongoDB&quot;], [&quot;libsvm&quot;, &quot;regression&quot;, &quot;support vector machines&quot;] ] . unique_interests = sorted({interest for user_interests in users_interests for interest in user_interests}) unique_interests . [&#39;Big Data&#39;, &#39;C++&#39;, &#39;Cassandra&#39;, &#39;HBase&#39;, &#39;Hadoop&#39;, &#39;Haskell&#39;, &#39;Java&#39;, &#39;Mahout&#39;, &#39;MapReduce&#39;, &#39;MongoDB&#39;, &#39;MySQL&#39;, &#39;NoSQL&#39;, &#39;Postgres&#39;, &#39;Python&#39;, &#39;R&#39;, &#39;Spark&#39;, &#39;Storm&#39;, &#39;artificial intelligence&#39;, &#39;databases&#39;, &#39;decision trees&#39;, &#39;deep learning&#39;, &#39;libsvm&#39;, &#39;machine learning&#39;, &#39;mathematics&#39;, &#39;neural networks&#39;, &#39;numpy&#39;, &#39;pandas&#39;, &#39;probability&#39;, &#39;programming languages&#39;, &#39;regression&#39;, &#39;scikit-learn&#39;, &#39;scipy&#39;, &#39;statistics&#39;, &#39;statsmodels&#39;, &#39;support vector machines&#39;, &#39;theory&#39;] . def make_user_interest_vector(user_interests): &quot;&quot;&quot; Given a list ofinterests, produce a vector whose its element is 1 if unique_interests[i] is in the list, 0 otherwise &quot;&quot;&quot; return [1 if interest in user_interests else 0 for interest in unique_interests] . user_interest_vectors = [make_user_interest_vector(user_interests) for user_interests in users_interests] len(user_interest_vectors) user_interest_vectors . [[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]] . Utils . import math def dot(v, w): &quot;&quot;&quot;Computes v_1 * w_1 + ... + v_n * w_n&quot;&quot;&quot; assert len(v) == len(w), &quot;vectors must be same length&quot; return sum(v_i * w_i for v_i, w_i in zip(v, w)) def cosine_similarity(v1, v2): return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2)) . user_similarities = [[ cosine_similarity(interest_vector_i, interest_vector_j) for interest_vector_j in user_interest_vectors] for interest_vector_i in user_interest_vectors] print(len(user_similarities)) print(user_similarities[0]) # similarity (user0&amp;user1) print(user_similarities[0][1]) print(user_similarities[1][0]) . 15 [1.0, 0.3380617018914066, 0.0, 0.0, 0.0, 0.1543033499620919, 0.0, 0.0, 0.1889822365046136, 0.5669467095138409, 0.0, 0.0, 0.0, 0.1690308509457033, 0.0] 0.3380617018914066 0.3380617018914066 . Show user similarity . print(users_interests[0], users_interests[9], user_similarities[0][9]) # user0, 8 share interest only &quot;Big Data&quot; print(users_interests[0], users_interests[8], user_similarities[0][8]) . [&#39;Hadoop&#39;, &#39;Big Data&#39;, &#39;HBase&#39;, &#39;Java&#39;, &#39;Spark&#39;, &#39;Storm&#39;, &#39;Cassandra&#39;] [&#39;Hadoop&#39;, &#39;Java&#39;, &#39;MapReduce&#39;, &#39;Big Data&#39;] 0.5669467095138409 [&#39;Hadoop&#39;, &#39;Big Data&#39;, &#39;HBase&#39;, &#39;Java&#39;, &#39;Spark&#39;, &#39;Storm&#39;, &#39;Cassandra&#39;] [&#39;neural networks&#39;, &#39;deep learning&#39;, &#39;Big Data&#39;, &#39;artificial intelligence&#39;] 0.1889822365046136 . def most_similar_users_to(user_id): # find other users with nonzero similality pairs = [(other_user_id, similarity) for other_user_id, similarity in enumerate(user_similarities[user_id]) if user_id != other_user_id and similarity &gt; 0] # sort them most similar first return sorted(pairs, key = lambda pair: pair[-1], reverse = True) . most_similar_users_to(0) . [(9, 0.5669467095138409), (1, 0.3380617018914066), (8, 0.1889822365046136), (13, 0.1690308509457033), (5, 0.1543033499620919)] . user0 is similar to 9. cosaine similarity is about 0.56 . suggest recomendation . sum up the similarities every each domain . from collections import defaultdict def user_based_suggestions(user_id, include_current_interests=False): # Sum up the similarities. suggestions: Dict[str, float] = defaultdict(float) for other_user_id, similarity in most_similar_users_to(user_id): for interest in users_interests[other_user_id]: suggestions[interest] += similarity # Convert them to a sorted list. suggestions = sorted(suggestions.items(), key=lambda pair: pair[-1], # weight reverse=True) # And (maybe) exclude already-interests if include_current_interests: return suggestions else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] . ubs0 = user_based_suggestions(0) ubs0 . [(&#39;MapReduce&#39;, 0.5669467095138409), (&#39;MongoDB&#39;, 0.50709255283711), (&#39;Postgres&#39;, 0.50709255283711), (&#39;NoSQL&#39;, 0.3380617018914066), (&#39;neural networks&#39;, 0.1889822365046136), (&#39;deep learning&#39;, 0.1889822365046136), (&#39;artificial intelligence&#39;, 0.1889822365046136), (&#39;databases&#39;, 0.1690308509457033), (&#39;MySQL&#39;, 0.1690308509457033), (&#39;Python&#39;, 0.1543033499620919), (&#39;R&#39;, 0.1543033499620919), (&#39;C++&#39;, 0.1543033499620919), (&#39;Haskell&#39;, 0.1543033499620919), (&#39;programming languages&#39;, 0.1543033499620919)] . Weak point user based collaborative filtering . if number of elements increase , then don&#39;t sugget well for curse of dimensionality üëâ then content base try to solve its problem | . | . item based collaborative filtering . unique_interests[0] . &#39;Big Data&#39; . interest_user_matrix = [[user_interest_vector[j] for user_interest_vector in user_interest_vectors] for j, _ in enumerate(unique_interests)] . print(unique_interests[0]) # ex) user 0, 8, 9 has interest &quot;Big Data&quot; interest_user_matrix . Big Data . [[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]] . if same users are interested in 2 items , cosaine similarity is 1, if not similarity is 0 . interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j) for user_vector_j in interest_user_matrix] for user_vector_i in interest_user_matrix] . interest_similarities[0] . [1.0, 0.0, 0.4082482904638631, 0.3333333333333333, 0.8164965809277261, 0.0, 0.6666666666666666, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.5773502691896258, 0.4082482904638631, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] . def most_similar_interests_to(interest_id): similarities = interest_similarities[interest_id] pairs = [(unique_interests[other_interest_id], similarity) for other_interest_id, similarity in enumerate(similarities) if interest_id != other_interest_id and similarity &gt; 0] return sorted(pairs, key=lambda pair: pair[-1], reverse=True) . for example, similarity with &quot;Big Data&quot; is below . msit0 = most_similar_interests_to(0) msit0 . [(&#39;Hadoop&#39;, 0.8164965809277261), (&#39;Java&#39;, 0.6666666666666666), (&#39;MapReduce&#39;, 0.5773502691896258), (&#39;Spark&#39;, 0.5773502691896258), (&#39;Storm&#39;, 0.5773502691896258), (&#39;Cassandra&#39;, 0.4082482904638631), (&#39;artificial intelligence&#39;, 0.4082482904638631), (&#39;deep learning&#39;, 0.4082482904638631), (&#39;neural networks&#39;, 0.4082482904638631), (&#39;HBase&#39;, 0.3333333333333333)] . for interest, similarity in most_similar_interests_to(0): print(interest, similarity) . Hadoop 0.8164965809277261 Java 0.6666666666666666 MapReduce 0.5773502691896258 Spark 0.5773502691896258 Storm 0.5773502691896258 Cassandra 0.4082482904638631 artificial intelligence 0.4082482904638631 deep learning 0.4082482904638631 neural networks 0.4082482904638631 HBase 0.3333333333333333 . def item_based_suggestions(user_id, include_current_interests=False): # Add up the similar interests suggestions = defaultdict(float) user_interest_vector = user_interest_vectors[user_id] for interest_id, is_interested in enumerate(user_interest_vector): similar_interests = most_similar_interests_to(interest_id) for interest, similarity in similar_interests: suggestions[interest] += similarity # Sort them by weight suggestions = sorted(suggestions.items(), key=lambda pair: pair[-1], reverse=True) print(&quot;suggestion is : {}&quot;.format(suggestions)) # And (maybe) exclude already-interests if include_current_interests: return suggestion else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] . recommendation to user0 . ibs0 = item_based_suggestions(0) ibs0 . suggestion is : [(&#39;HBase&#39;, 6.411156045861838), (&#39;Java&#39;, 6.097094776993208), (&#39;Python&#39;, 5.672467491345621), (&#39;Cassandra&#39;, 5.2543135054150945), (&#39;Big Data&#39;, 5.1815405503520555), (&#39;Hadoop&#39;, 4.662561795878958), (&#39;scikit-learn&#39;, 4.474873734152916), (&#39;MongoDB&#39;, 4.437816924487368), (&#39;Postgres&#39;, 4.437816924487368), (&#39;R&#39;, 4.2540358447538855), (&#39;Spark&#39;, 4.146264369941973), (&#39;Storm&#39;, 4.146264369941973), (&#39;probability&#39;, 3.9567956789604666), (&#39;C++&#39;, 3.9378169244873686), (&#39;regression&#39;, 3.6234623456271335), (&#39;numpy&#39;, 3.6213203435596424), (&#39;scipy&#39;, 3.6213203435596424), (&#39;statsmodels&#39;, 3.5295686340235055), (&#39;pandas&#39;, 3.4748737341529163), (&#39;statistics&#39;, 3.4289742326275534), (&#39;machine learning&#39;, 3.322461852836958), (&#39;Haskell&#39;, 3.284457050376173), (&#39;programming languages&#39;, 3.284457050376173), (&#39;neural networks&#39;, 3.1153550716504106), (&#39;MySQL&#39;, 2.9915638315627207), (&#39;databases&#39;, 2.9915638315627207), (&#39;artificial intelligence&#39;, 2.8164965809277263), (&#39;deep learning&#39;, 2.8164965809277263), (&#39;libsvm&#39;, 2.730710143300821), (&#39;NoSQL&#39;, 2.698670612749268), (&#39;theory&#39;, 2.1547005383792515), (&#39;mathematics&#39;, 2.1547005383792515), (&#39;Mahout&#39;, 2.1213203435596424), (&#39;decision trees&#39;, 1.9915638315627207), (&#39;MapReduce&#39;, 1.861807319565799), (&#39;support vector machines&#39;, 1.2844570503761732)] . [(&#39;Python&#39;, 5.672467491345621), (&#39;scikit-learn&#39;, 4.474873734152916), (&#39;MongoDB&#39;, 4.437816924487368), (&#39;Postgres&#39;, 4.437816924487368), (&#39;R&#39;, 4.2540358447538855), (&#39;probability&#39;, 3.9567956789604666), (&#39;C++&#39;, 3.9378169244873686), (&#39;regression&#39;, 3.6234623456271335), (&#39;numpy&#39;, 3.6213203435596424), (&#39;scipy&#39;, 3.6213203435596424), (&#39;statsmodels&#39;, 3.5295686340235055), (&#39;pandas&#39;, 3.4748737341529163), (&#39;statistics&#39;, 3.4289742326275534), (&#39;machine learning&#39;, 3.322461852836958), (&#39;Haskell&#39;, 3.284457050376173), (&#39;programming languages&#39;, 3.284457050376173), (&#39;neural networks&#39;, 3.1153550716504106), (&#39;MySQL&#39;, 2.9915638315627207), (&#39;databases&#39;, 2.9915638315627207), (&#39;artificial intelligence&#39;, 2.8164965809277263), (&#39;deep learning&#39;, 2.8164965809277263), (&#39;libsvm&#39;, 2.730710143300821), (&#39;NoSQL&#39;, 2.698670612749268), (&#39;theory&#39;, 2.1547005383792515), (&#39;mathematics&#39;, 2.1547005383792515), (&#39;Mahout&#39;, 2.1213203435596424), (&#39;decision trees&#39;, 1.9915638315627207), (&#39;MapReduce&#39;, 1.861807319565799), (&#39;support vector machines&#39;, 1.2844570503761732)] . REF . Á¨¨3Âõû„ÄÄ„Éô„Ç§„Ç∏„Ç¢„É≥„Éï„Ç£„É´„Çø„ÇíÂÆüË£Ö„Åó„Å¶„Åø„Çà„ÅÜ | data-science-from-scratch | .",
            "url": "https://interkid.github.io/My_Research/2021/10/30/RecommendSystem.html",
            "relUrl": "/2021/10/30/RecommendSystem.html",
            "date": " ‚Ä¢ Oct 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Math for Data Science",
            "content": "!pip install d2l !pip install -U pandas_profiling import os os.kill(os.getpid(), 9) . Collecting d2l Downloading d2l-0.17.0-py3-none-any.whl (83 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83 kB 1.5 MB/s Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5) Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2) Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.3.1) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.6.1) Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.1.1) Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.2.0) Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (7.6.5) Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (4.10.1) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.1.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.3.5) Requirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.5.0) Requirement already satisfied: traitlets&gt;=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.1.0) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.8.1) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.7.5) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (1.0.18) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (4.8.0) Requirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (57.4.0) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (4.4.2) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (2.6.1) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.2.5) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (1.15.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (1.0.2) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (0.2.0) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (3.5.1) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (5.1.3) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;jupyter-&gt;d2l) (2.6.0) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;jupyter-&gt;d2l) (4.8.1) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (0.12.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (1.8.0) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (2.11.3) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel-&gt;jupyter-&gt;d2l) (2.8.2) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel-&gt;jupyter-&gt;d2l) (22.3.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook-&gt;jupyter-&gt;d2l) (0.7.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;notebook-&gt;jupyter-&gt;d2l) (2.0.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (0.10.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (2.4.7) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.8.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.3) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.5.0) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (4.1.0) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (1.5.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.7.1) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;d2l) (21.0) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;d2l) (0.5.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;d2l) (2018.9) Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole-&gt;jupyter-&gt;d2l) (1.11.2) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (2021.5.30) Installing collected packages: d2l Successfully installed d2l-0.17.0 Requirement already satisfied: pandas_profiling in /usr/local/lib/python3.7/dist-packages (1.4.1) Collecting pandas_profiling Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 261 kB 5.4 MB/s Collecting multimethod&gt;=1.4 Downloading multimethod-1.6-py3-none-any.whl (9.4 kB) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.19.5) Requirement already satisfied: seaborn&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (0.11.2) Requirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (2.11.3) Requirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (2.0.1) Collecting visions[type_image_path]==0.7.4 Downloading visions-0.7.4-py3-none-any.whl (102 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102 kB 12.8 MB/s Collecting PyYAML&gt;=5.0.0 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 49.3 MB/s Requirement already satisfied: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.4.1) Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.1.5) Collecting htmlmin&gt;=0.1.12 Downloading htmlmin-0.1.12.tar.gz (19 kB) Requirement already satisfied: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (4.62.3) Requirement already satisfied: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (3.2.2) Collecting phik&gt;=0.11.1 Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 675 kB 46.4 MB/s Requirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (0.5.0) Collecting tangled-up-in-unicode==0.1.0 Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.1 MB 56.0 MB/s Collecting pydantic&gt;=1.8.1 Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.1 MB 77.4 MB/s Requirement already satisfied: joblib~=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.0.1) Collecting requests&gt;=2.24.0 Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62 kB 849 kB/s Requirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas_profiling) (2.6.3) Requirement already satisfied: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas_profiling) (21.2.0) Collecting imagehash Downloading ImageHash-4.2.1.tar.gz (812 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 812 kB 83.8 MB/s Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas_profiling) (7.1.2) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (2.8.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (0.10.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.2.0-&gt;pandas_profiling) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas_profiling) (2018.9) Collecting scipy&gt;=1.4.1 Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.5 MB 49 kB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas_profiling) (3.7.4.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (2021.5.30) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (1.24.3) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (2.0.6) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (2.10) Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.7.4-&gt;pandas_profiling) (1.1.1) Building wheels for collected packages: htmlmin, imagehash Building wheel for htmlmin (setup.py) ... done Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=b7c3b2b05d41f3398f13252e866d4fe387bbf6ca410bc70584eac03fa0a9ee4d Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655 Building wheel for imagehash (setup.py) ... done Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295207 sha256=ac7c7cb85efa8bcc38664c9961891f32b4b301615131d775bfefa85f230789a1 Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e Successfully built htmlmin imagehash Installing collected packages: tangled-up-in-unicode, scipy, multimethod, visions, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: requests Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: pandas-profiling Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible. Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 multimethod-1.6 pandas-profiling-3.1.0 phik-0.12.0 pydantic-1.8.2 requests-2.26.0 scipy-1.7.1 tangled-up-in-unicode-0.1.0 visions-0.7.4 . Part 1 Theoretical explanation . Differential calculus . The best constant approximation for the rate of change . . $L = lim_{h‚Üí0} frac{f(a+h) - f(a)}{h}$ . Power rules $ frac{d}{dx}x^a=ax^{a-1}$ . | Exponential and logarthm $ frac{d}{dx}e^x=e^x$ $ frac{d}{dx}a^x=a^xln(a)$ . | Sum rule $(af+ beta g)` = af` + beta g`$ . | Product rule $(fg)` = f`g +fg`$ . | Quotient rule $( frac{f}{g})` = frac{f`g-fg`}{g^2}$ . | . Integral Calculus . an integral assigns numbers to functions in a way that describes displacement, area, volume, and other concepts that arise by combining infinitesimal data . Application: EM Algorithm . ${ displaystyle L({ boldsymbol { theta }}; mathbf {X} )=p( mathbf {X} mid { boldsymbol { theta }})= int p( mathbf {X} , mathbf {Z} mid { boldsymbol { theta }}) ,d mathbf {Z} = int p( mathbf {Z} mid mathbf {X} ,{ boldsymbol { theta }})p( mathbf {X} mid { boldsymbol { theta }}) ,d mathbf {Z} }$ ${ displaystyle Q({ boldsymbol { theta }} mid { boldsymbol { theta }}^{(t)})= operatorname {E} _{ mathbf {Z} mid mathbf {X} ,{ boldsymbol { theta }}^{(t)}} left[ log L({ boldsymbol { theta }}; mathbf {X} , mathbf {Z} ) right] ,}$ . %matplotlib inline import torch from IPython import display from mpl_toolkits import mplot3d from d2l import torch as d2l x = torch.arange(-2, 2, 0.01) f = torch.exp(-x**2) d2l.set_figsize() d2l.plt.plot(x, f, color=&#39;black&#39;) d2l.plt.fill_between(x.tolist(), f.tolist()) d2l.plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; x = torch.arange(-2, 2, 0.01) f = torch.exp(-x**2) d2l.set_figsize() d2l.plt.plot(x, f, color=&#39;black&#39;) d2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250]) d2l.plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; from scipy import stats distribution = &quot;normal&quot; #@param [&quot;normal&quot;, &quot;gamma&quot;, &quot;uniform&quot;, &quot;laplace&quot;, &quot;double weibull&quot;] {allow-input: true} epsilon = 0.25224 #@param {type:&quot;slider&quot;, min:0, max:1, step:0.00001} min = -8 #@param {type:&quot;number&quot;} max = 8#@param {type:&quot;number&quot;} x = torch.arange(min, max, epsilon) #f = x / (1 + x**2) if distribution == &quot;normal&quot;: f=torch.tensor(stats.norm(0, 1).pdf(x)) elif distribution == &quot;gamma&quot;: f = torch.tensor(stats.gamma.pdf(x, 0.6)) elif distribution == &quot;uniform&quot;: f = torch.tensor(stats.uniform.pdf(x)) elif distribution == &quot;laplace&quot;: f = torch.tensor(stats.laplace.pdf(x)) elif distribution == &quot;double weibull&quot;: f = torch.tensor(stats.dweibull.pdf(x, 2.07)) approx = torch.sum(epsilon * f) #true = torch.log(torch.tensor([5.])) / 2 d2l.set_figsize() d2l.plt.bar(x, f, width=epsilon, align=&#39;edge&#39;) d2l.plt.plot(x, f, color=&#39;black&#39;) d2l.plt.show() f&#39;approximation: {approx}&#39; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; &#39;approximation: 1.0000000130791062&#39; . Gradient Descent Algorithm . . . Gradient Descent minimize the loss between objective and cost function by taking partial derivatives with respect to each parameter(w, b in this example) . „ÉªGradient $ nabla L = ( frac{ partial L}{ partial w} frac{ partial L}{ partial b})$ . „ÉªPartial Derivatives $ frac{ partial}{ partial w}((y-(wx^2+b)) = -2x^2(-b-wx^2+y)$ $ frac{ partial}{ partial b}((y-(wx^2+b)) = -2(-b-wx^2+y)$ . „ÉªUpdate parameters $w ‚Üê w - eta frac{ partial f}{ partial w} $ $b ‚Üê b - eta frac{ partial f}{ partial b} $ . Neural Network . Reference . Math for Data Science - Online Workshop | .",
            "url": "https://interkid.github.io/My_Research/2021/10/27/MathNotebook.html",
            "relUrl": "/2021/10/27/MathNotebook.html",
            "date": " ‚Ä¢ Oct 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "install surf2stl-python . !git clone https://github.com/asahidari/surf2stl-python . Cloning into &#39;surf2stl-python&#39;... remote: Enumerating objects: 19, done. remote: Counting objects: 100% (19/19), done. remote: Compressing objects: 100% (16/16), done. remote: Total 19 (delta 4), reused 17 (delta 2), pack-reused 0 Unpacking objects: 100% (19/19), done. . cd surf2stl-python . /content/surf2stl-python . How to draw CalbiYau Manifold . $$ z^n_1 + z^n_2 = 1 $$$$ z_1= e^{iœÜ}[cos(x + iy)]^ frac2{n} $$$$ z_2= e^{iœÜ}[sin (x + iy)]^ frac2{n} $$$$ œÜ_1= frac{2œÄk_1}{n} (0 ‚â¶ k &lt; n) $$$$ œÜ_2= frac{2œÄk_1}{n} (0 ‚â¶ k &lt; n) $$ . Parameter k1 and k2 individually take Integer values from 0 to n - 1, and results in n x n parts of the manifold(manupulate x, y each pattern in $nxn=n^2$) . | to visualize Calabi-Yau manifold means that to satisfy equation $z^n_1 + z^n_2 = 1$ ,then we can get z1, z2 by moving parameter x,y and integer k1, k2 . | $z_1, z_2$ spread 4Dimention thinking considering real number&amp;imaginary number($Re(z_1),Im(z_1),Re(z_2),Im(z_2)$ ), so we should think to reduce dimention | reduce $Im(z_1),Im(z_2)$ then make $(Re(z1),Re(z2),Im(z1)cos(a)+Im(z2)sin(a))$ 3D | . Import library . import numpy as np import math, cmath # cmath: Ë§áÁ¥†Êï∞„ÅÆ„Åü„ÇÅ„ÅÆ„É©„Ç§„Éñ„É©„É™ import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D import matplotlib.ticker as plticker from matplotlib import cm from scipy.spatial import Delaunay import surf2stl as s2s . Set Parameter . N = 9 # Dimension a = 0.4 row, col = 30, 30 writeSTL = False . define function for caculation . def calcZ1(x, y, k, n): return cmath.exp(1j*(2*cmath.pi*k/n)) * (cmath.cos(x+y*1j)**(2/n)) def calcZ2(x, y, k, n): return cmath.exp(1j*(2*cmath.pi*k/n)) * (cmath.sin(x+y*1j)**(2/n)) def calcZ1Real(x, y, k, n): return (calcZ1(x, y, k, n)).real def calcZ2Real(x, y, k, n): return (calcZ2(x, y, k, n)).real def calcZ(x, y, k1_, k2_, n, a_): z1 = calcZ1(x, y, k1, n) z2 = calcZ2(x, y, k2, n) return z1.imag * math.cos(a_) + z2.imag*math.sin(a_) . Draw . x = np.linspace(0, math.pi/2, col) y = np.linspace(-math.pi/2, math.pi/2, row) x, y = np.meshgrid(x, y) # init graph fig = plt.figure(figsize=(18,8)) for n in range(2, N): ax = fig.add_subplot(2, 4, n - 1, projection=&#39;3d&#39;) ax.view_init(elev=15, azim=15) ax.set_title(&quot;n=%d&quot; % n) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals ax.xaxis.set_major_locator(loc) ax.yaxis.set_major_locator(loc) ax.zaxis.set_major_locator(loc) count = 0 for k1 in range(n): for k2 in range(n): # calc X, Y, Z values X = np.frompyfunc(calcZ1Real, 4, 1)(x, y, k1, n).astype(&#39;float32&#39;) Y = np.frompyfunc(calcZ2Real, 4, 1)(x, y, k2, n).astype(&#39;float32&#39;) Z = np.frompyfunc(calcZ, 6, 1)(x, y, k1, k2, n, a).astype(&#39;float32&#39;) ax.plot_surface(X, Y, Z, cmap=cm.ocean, linewidth=0) . Reference . Creating Calabi Yau Manifold in python | Calabi-YauÂ§öÊßò‰Ωì„Çí„Éñ„É©„Ç¶„Ç∂‰∏ä„Å´ÂèØË¶ñÂåñ„Åô„Çã | .",
            "url": "https://interkid.github.io/My_Research/2021/10/27/CalabiYauManifold.html",
            "relUrl": "/2021/10/27/CalabiYauManifold.html",
            "date": " ‚Ä¢ Oct 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "NewworkX„Å´„Çà„ÇãÂèØË¶ñÂåñ/ÂàÜÊûê",
            "content": "&#30446;&#27425; . Âü∫Êú¨ | ÂØÜÂ∫¶„Å®„ÇØ„É©„Çπ„Çø‰øÇÊï∞ | „Çπ„É¢„Éº„É´„ÉØ„Éº„É´„ÉâÁîüÊàê | Ê¨°Êï∞„Å®Ê¨°Êï∞ÂàÜÂ∏É | „Çπ„Ç±„Éº„É´„Éï„É™„Éº‰ΩúÊàê | Ë∑ùÈõ¢„Å®Áõ¥ÂæÑ | ‰∏≠ÂøÉÊÄß | . NetworkX„Åß„Åß„Åç„Çã„Åì„Å® . „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Ç∞„É©„Éï‰ΩúÊàê„ÅÆ„Ç≥„Éº„Éâ„Å´„Çà„ÇãÁ∞°Âçò„Å™ÁîüÊàê „Ç∞„É©„Éï„ÅÆ„Éì„Ç∏„É•„Ç¢„É´„ÅØ„Ç∑„É≥„Éó„É´(Á∂∫È∫ó„Å™„É¨„Éô„É´„Åß„ÅØ„Å™„ÅÑ) | . | „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂàÜÊûê„ÅÆÊñáËÑà„Åß„ÅÆÁî®ÈÄî„ÅåÂ§ö„ÅÑ ex. ‰∏≠ÂøÉ‰∫∫Áâ©(„Éè„Éñ)„ÇíË¶ã„Å§„Åë„Å¶ÊÑüÊüìÂà∂Âæ°, „Ç∞„É´„Éº„Éó„ÇíÊäΩÂá∫„Åó„Å¶„ÄÅÊé®Ëñ¶„Ç∑„Çπ„ÉÜ„É† „Å™„Å© | . | . „Åì„ÅÆË®ò‰∫ã„ÅÆÂèÇËÄÉ„ÅØ„Åª„Åº„Åì„Å°„ÇâÂºïÁî®üôè https://www.youtube.com/watch?v=AC4u1PYlveU | ÂÖ¨Âºèref https://networkx.org/documentation/stable/reference/index.html | . &#22522;&#26412; . import networkx as nx # NetworkX„Çí„Ç§„É≥„Éù„Éº„Éà import matplotlib.pyplot as plt import numpy as np . G = nx.Graph([(1, 2), (2, 3), (3, 1)]) # G = nx.Graph() # Á©∫„ÅÆ„Ç∞„É©„Éï„Çí‰ΩúÊàê„Åô„ÇãÂ†¥Âêà nx.draw(G, with_labels=True,font_family=&#39;sans-serif&#39;) # „É©„Éô„É´„ÇíTrue„Å´„Åó„Å¶Áï™Âè∑„ÅÆÂèØË¶ñÂåñ . Êï∞Â≠ó‰ª•Â§ñ„ÅÆÊó•Êú¨Ë™û,ÊñáÂ≠ó„ÅÆË®òËø∞„ÇÇË®≠ÂÆö„ÇíÂä†„Åà„Çå„Å∞ÂèØËÉΩ(default„ÅßÊñáÂ≠óÂåñ„Åë„Åô„Çã) | . G.add_node(4) # 4„ÅØÁπã„Åå„Å£„Å¶„Å™„ÅÑ„ÅÆ„ÅßÈÅ†„Åè„Å´ÈÖçÁΩÆ nx.draw(G, with_labels=True) . G.add_edge(1,4) nx.draw(G, with_labels=True) . n = [5, 6, 7] G.add_nodes_from(n) # Ë§áÊï∞„É™„É≥„ÇØ„ÅÆËøΩÂä† e = [(5,6), (3,7), (4,6)] G.add_edges_from(e) nx.draw(G, with_labels=True) . G.remove_node(4) nx.draw(G, with_labels=True) . G.remove_edges_from([(1,2), (2,3)]) nx.draw(G, with_labels=True) . rnd = nx.gnp_random_graph(10, 0.5) # (Á¨¨1ÂºïÊï∞„ÄÄ„Éé„Éº„Éâ„ÅÆÊï∞, Á¨¨ÔºíÂºïÊï∞„ÄÄ„Ç®„ÉÉ„Ç∏„ÇíÂºï„ÅèÁ¢∫Áéá(1„ÅßÂÖ®„Å¶Âºï„Åè)) pos=nx.circular_layout(rnd) # ÂÜÜÂë®‰∏ä„Å´Á≠âÈñìÈöî„ÅßÈÖçÁΩÆ #pos = nx.spring_layout(rnd) # „Éê„Éç„É¢„Éá„É´(Áπã„Åå„Çä(„Éê„Éç)„ÅåËá™ÁÑ∂„ÅÆÈï∑„Åï) #pos = nx.random_layout(rnd) #„É©„É≥„ÉÄ„É†„Å™„É¨„Ç§„Ç¢„Ç¶„Éà plt.title(&#39;random graph&#39;) nx.draw(rnd, pos, with_labels=True) . K_10 = nx.complete_graph(10) pos=nx.spring_layout(K_10) plt.title(&#39;complete graph&#39;) nx.draw(K_10, pos, with_labels=True) . star = nx.star_graph(100) pos=nx.spring_layout(star) plt.title(&#39;star graph&#39;) nx.draw(star, pos, with_labels=True) . wheel = nx.wheel_graph(10) pos=nx.spring_layout(wheel) plt.title(&#39;wheel graph&#39;) nx.draw(wheel, pos, with_labels=True) . &#23494;&#24230;&#12392;&#12463;&#12521;&#12473;&#12479;&#20418;&#25968; . ÂØÜÂ∫¶: „Å©„Çå„Å†„Åë„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅåÂØÜÈõÜ„Åó„Å¶„ÅÑ„Çã„Åã . ÂØÜÂ∫¶„ÇíÊ±Ç„ÇÅ„ÇãÂºè $$ density = „É™„É≥„ÇØ„ÅÆÁ∑èÊï∞ / „Éé„Éº„Éâ„ÅÆ„Éö„Ç¢„ÅÆÁ∑èÊï∞ = frac{m}{n(n-1) -2} $$ . %matplotlib inline import networkx as nx # NetworkX„Çí„Ç§„É≥„Éù„Éº„Éà import matplotlib.pyplot as plt import numpy as np # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÁîüÊàê G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (3, 4), (4,5), (4,6),(5,6)]) nx.draw(G, with_labels=True) # „É©„Éô„É´„ÇíTrue„Å´„Åó„Å¶Áï™Âè∑„ÅÆÂèØË¶ñÂåñ . „ÇØ„É©„Çπ„Çø‰øÇÊï∞ : Èö£Êé•„Éé„Éº„ÉâÂêåÂ£´„Åå„Å©„ÅÆ„Åè„Çâ„ÅÑÁπã„Åå„Å£„Å¶„ÅÑ„Çã„Åã (Êï∞ÂÄ§Âåñ„ÅÆÊï∞Âºè„ÅØÁúÅÁï•) . print(nx.average_clustering(G)) . 0.5833333333333334 . %matplotlib inline import networkx as nx # NetworkX„Çí„Ç§„É≥„Éù„Éº„Éà import matplotlib.pyplot as plt import numpy as np # „É©„É≥„ÉÄ„É†„Ç∞„É©„ÉïÁîüÊàê rnd = nx.gnp_random_graph(10, 0.1) #pos=nx.spring_layout(rnd) pos=nx.circular_layout(rnd) #pos = nx.random_layout(rnd) plt.title(&#39;random graph&#39;) nx.draw(rnd, pos, with_labels=True) print(&#39;density:&#39;, nx.density(rnd), &#39;, clustering coefficient:&#39;, nx.average_clustering(rnd)) . density: 0.044444444444444446 , clustering coefficient: 0.0 . cycle = nx.cycle_graph(10) pos=nx.spring_layout(cycle) #pos=nx.circular_layout(cycle) #pos=nx.random_layout(cycle) plt.title(&#39;cycle graph&#39;) nx.draw(cycle, pos, with_labels=True) print(&#39;density:&#39;, nx.density(cycle), &#39;, clustering coefficient:&#39;, nx.average_clustering(cycle)) . density: 0.2222222222222222 , clustering coefficient: 0.0 . K_10 = nx.complete_graph(10) pos=nx.spring_layout(K_10) plt.title(&#39;complete graph&#39;) nx.draw(K_10, pos, with_labels=True) print(&#39;density:&#39;, nx.density(K_10), &#39;, clustering coefficient:&#39;, nx.average_clustering(K_10)) . density: 1.0 , clustering coefficient: 1.0 . &#12473;&#12514;&#12540;&#12523;&#12527;&#12540;&#12523;&#12489;&#12493;&#12483;&#12488;&#12527;&#12540;&#12463;&#29983;&#25104; . ÁµåË∑ØÈï∑„ÅåÈï∑„ÅÑ&amp; È´òÂ∫¶„Å´„ÇØ„É©„Çπ„ÇøÂåñ„ÇíÂÖº„Å≠ÂÇô„Åà„Åü„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ . n = 100 # „Éé„Éº„ÉâÊï∞ k = 4 # Ê¨°Êï∞ p = 0.1 # „É™„É≥„ÇØ„Å§„Å™„ÅéÊõø„ÅàÁ¢∫Áéá print(&#39;NetworkX„ÅÆwatts_strogatz_graph()„Çà„ÇäÁîüÊàê&#39;) G1 = nx.watts_strogatz_graph(n, k, p) pos = nx.circular_layout(G1) print(nx.info(G1)) # Á®Æ„ÄÖ„ÅÆÊÉÖÂ†±„ÇíÂá∫Âäõ print(&#39;„ÇØ„É©„Çπ„Çø‰øÇÊï∞Ôºö&#39;, nx.average_clustering(G1)) # „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂÖ®‰Ωì„ÅÆ„ÇØ„É©„Çπ„Çø‰øÇÊï∞„ÇíÂá∫Âäõ nx.draw(G1, pos) plt.show() . NetworkX„ÅÆwatts_strogatz_graph()„Çà„ÇäÁîüÊàê Graph with 100 nodes and 200 edges „ÇØ„É©„Çπ„Çø‰øÇÊï∞Ôºö 0.42566666666666664 . def gen_WS_network(n, k, p, seed=None): if seed is not None: np.random.seed(seed=seed) G = nx.Graph() G.add_nodes_from(list(range(n))) for i in range(n): for j in range(k//2): G.add_edge(i, (i+j+1)%n) # k/2Êú¨„É™„É≥„ÇØ„ÇíÂºµ„Çã for (u,v) in G.edges(): if np.random.rand() &lt; p: # p„ÅÆÁ¢∫Áéá„Åß„Å§„Å™„ÅéÊõø„Åà G.remove_edge(u, v) new_node = (u+np.random.randint(n-1)+1)%n # Êñ∞„Åó„ÅÑÊé•Á∂öÂÖà„Å®„Åó„Å¶u+1„Åã„Çâu-1„Åæ„Åß„ÅÆ‰∏≠„Åã„Çâ„É©„É≥„ÉÄ„É†„Å´ÈÅ∏„Å∂ while G.has_edge(u, new_node) == True: # Êó¢Â≠ò„É™„É≥„ÇØ„ÅØÈô§Â§ñ new_node = (u+np.random.randint(n-1)+1)%n G.add_edge(u, new_node) return G . n = 100 k = 4 p = 0.1 print(&#39;Ëá™‰ΩúÈñ¢Êï∞„Å´„Çà„ÇãÁîüÊàê&#39;) G2 = gen_WS_network(n, k, p) pos = nx.circular_layout(G2) print(nx.info(G2)) print(&#39;„ÇØ„É©„Çπ„Çø‰øÇÊï∞Ôºö&#39;, nx.average_clustering(G2)) nx.draw(G2, pos) plt.show() . Ëá™‰ΩúÈñ¢Êï∞„Å´„Çà„ÇãÁîüÊàê Graph with 100 nodes and 200 edges „ÇØ„É©„Çπ„Çø‰øÇÊï∞Ôºö 0.3556190476190476 . &#27425;&#25968;&#12392;&#27425;&#25968;&#20998;&#24067; . Ê¨°Êï∞: „Éé„Éº„Éâ(È†ÇÁÇπ)„ÅåÊåÅ„Å§„É™„É≥„ÇØÔºàËæ∫)„ÅÆÊï∞ Âπ≥ÂùáÊ¨°Êï∞: Ê¨°Êï∞k„ÅÆÂπ≥ÂùáÂÄ§ Ê¨°Êï∞ÂàÜÂ∏É: Ê¨°Êï∞„ÅåÂÖ®„Éé„Éº„Éâ„Å´Âç†„ÇÅ„ÇãÂâ≤Âêà: p(k) Ê£í„Ç∞„É©„Éï„ÅßË°®„Åô„Åì„Å®„ÅåÂ§ö„ÅÑ . G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (4,5)]) print(nx.info(G)) print(G.nodes()) nx.draw(G, with_labels=True) # „É©„Éô„É´„ÇíTrue„Å´„Åó„Å¶Áï™Âè∑„ÅÆÂèØË¶ñÂåñ . Graph with 5 nodes and 5 edges [1, 2, 3, 4, 5] . print(nx.degree_histogram(G)) # ÊôÇÊï∞„ÅÆÂÄãÊï∞„ÇíÂá∫Âäõ degree_dist = [i/5 for i in nx.degree_histogram(G)] plt.rcParams[&#39;font.size&#39;] = 20 plt.bar(range(5), height = degree_dist) # ÊôÇÊï∞„ÅÆÂâ≤Âêà„Éí„Çπ„Éà„Ç∞„É©„É†„ÇíÂá∫Âäõ plt.xlabel(&#39;$k$&#39;) plt.ylabel(&#39;$p(k)$&#39;) plt.ylim(0,1) . [0, 2, 2, 0, 1] . (0.0, 1.0) . &#12473;&#12465;&#12540;&#12523;&#12501;&#12522;&#12540;&#12493;&#12483;&#12488;&#12527;&#12540;&#12463; . Â§öÊï∞„ÅåÂ∞ëÊï∞„Å®„Å§„Å™„Åå„Çä„ÄÅÂ∞ëÊï∞„ÅåÂ§öÊï∞„Å®„Å§„Å™„Åå„Å£„Å¶„ÅÑ„Çã„Çπ„Ç±„Éº„É´„Éï„É™„Éº„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ(BA„É¢„Éá„É´„Çí‰Ωø„ÅÜ) ex WWW, ‰ø≥ÂÑ™„ÅÆÂÖ±ÊºîÈñ¢‰øÇ ÊôÇÊï∞ÂàÜÂ∏É„ÅØ„Åπ„ÅçÂâá„Å®„Å™„Çã ÂèÇËÄÉ: https://syodokukai.exblog.jp/20771928/ . n = 100 m = 4 print(&#39;NetworkX.barabasi_albert_graph()&#39;) G1 = nx.barabasi_albert_graph(n, m) print(nx.info(G1)) nx.draw(G1) plt.show() . NetworkX.barabasi_albert_graph() Graph with 100 nodes and 384 edges . n = 5000 m = 4 # „Çπ„Ç±„Éº„É´„Éï„É™„Éº„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÁîüÊàê G = nx.barabasi_albert_graph(n, m) # ÁµÑ„ÅøËæº„ÅøÈñ¢Êï∞„ÅßÁîüÊàê # G = gen_BA_network(n, m) # Ëá™‰ΩúÈñ¢Êï∞„ÅßÁîüÊàê k = [i for i,x in enumerate(nx.degree_histogram(G)) if x != 0] degree_dist = [i/n for i in nx.degree_histogram(G) if i != 0] print(k) print(degree_dist) plt.xscale(&#39;log&#39;) plt.yscale(&#39;log&#39;) plt.ylim(0.0001,1) plt.scatter(k, degree_dist) . [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 56, 57, 62, 63, 65, 67, 72, 73, 74, 75, 76, 77, 81, 85, 87, 93, 94, 96, 97, 102, 107, 114, 115, 120, 122, 128, 201, 248, 267] [0.3324, 0.1938, 0.1154, 0.0776, 0.0542, 0.0396, 0.0308, 0.0254, 0.0188, 0.0152, 0.0118, 0.0106, 0.008, 0.007, 0.0056, 0.0048, 0.0052, 0.0034, 0.0038, 0.0024, 0.0028, 0.0014, 0.0018, 0.0012, 0.0022, 0.0016, 0.0018, 0.0012, 0.001, 0.0012, 0.0008, 0.0012, 0.0012, 0.0008, 0.0002, 0.001, 0.0006, 0.0012, 0.0002, 0.0004, 0.0006, 0.0004, 0.0004, 0.0002, 0.0004, 0.0006, 0.0004, 0.0004, 0.0004, 0.0002, 0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0004, 0.0004, 0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002] . &lt;matplotlib.collections.PathCollection at 0x7f1c8bddf050&gt; . &#36317;&#38626; . Ë∑ùÈõ¢: „Éé„Éº„Éâ„Éö„Ç¢Èñì„ÅÆÊúÄÁü≠ÁµåË∑Ø„ÅÆÈï∑„Åï | Âπ≥ÂùáË∑ùÈõ¢: ÂÖ®„Éö„Ç¢„ÅÆË∑ùÈõ¢„ÅÆÂπ≥ÂùáÂÄ§ | Áõ¥ÂæÑ: ÂÖ®„Éö„Ç¢„ÅÆË∑ùÈõ¢„ÅÆ‰∏≠„ÅßÊúÄÂ§ß„ÅÆ„ÇÇ„ÅÆ | . G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (3, 4), (4,5), (4,6),(5,6)]) nx.draw(G, with_labels=True) # „É©„Éô„É´„ÇíTrue„Å´„Åó„Å¶Áï™Âè∑„ÅÆÂèØË¶ñÂåñ . print(nx.shortest_path_length(G, 1, 2)) print(nx.shortest_path_length(G, 3, 6)) . 1 2 . print(nx.shortest_path(G, 1, 2)) print(nx.shortest_path(G, 2, 6)) print([p for p in nx.all_shortest_paths(G, 2, 6)]) . [1, 2] [2, 1, 4, 6] [[2, 1, 4, 6], [2, 1, 5, 6]] . print(&#39;L=&#39;, nx.average_shortest_path_length(G)) print(&#39;D=&#39;, nx.diameter(G)) . L= 1.5333333333333334 D= 3 . &#20013;&#24515;&#24615;(Centrality) . &quot;‰∏≠ÂøÉ&quot;„ÅÆÊåáÊ®ô . Ê¨°Êï∞‰∏≠ÂøÉÊÄß: Ê¨°Êï∞(„Éé„Éº„Éâ„ÅåÊåÅ„Å§„É™„É≥„ÇØ„ÅÆÊï∞)„ÅßÊ±∫„ÇÅ„Çã / Ê≠£Ë¶èÂåñ„Åó„ÅüÂÄ§„ÇíÁÆóÂá∫ | ËøëÊé•‰∏≠ÂøÉÊÄß: ‰ªñ„ÅÆÂÖ®„Å¶‰ªñ„ÅÆÂÖ®„Å¶„ÅÆ„Éé„Éº„Éâ„Å∏„ÅÆË∑ùÈõ¢(ÊúÄÁü≠ÁµåË∑ØÈï∑)„ÅÆÂπ≥ÂùáÂÄ§„ÅÆÈÄÜÊï∞ | ‰ªñ„Å´„ÇÇÊåáÊ®ô„ÅÇ„Çä | . Ê¥ªÁî®: sns„ÅÆ‰∏≠ÂøÉÊÄß„Å™„Å© . G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (3, 4), (4,5), (4,6),(5,6)]) nx.draw(G, with_labels=True) # „É©„Éô„É´„ÇíTrue„Å´„Åó„Å¶Áï™Âè∑„ÅÆÂèØË¶ñÂåñ print(&#39;Degree Centrality&#39;, nx.degree_centrality(G)) # Ê¨°Êï∞‰∏≠ÂøÉÊÄß print(&#39;Closeness Centrality&#39;, nx.closeness_centrality(G)) # ËøëÊé•‰∏≠ÂøÉÊÄß . Degree Centrality {1: 0.8, 2: 0.2, 3: 0.4, 4: 0.8, 5: 0.6000000000000001, 6: 0.4} Closeness Centrality {1: 0.8333333333333334, 2: 0.5, 3: 0.625, 4: 0.8333333333333334, 5: 0.7142857142857143, 6: 0.5555555555555556} .",
            "url": "https://interkid.github.io/My_Research/2021/10/20/NetworkVisualization.html",
            "relUrl": "/2021/10/20/NetworkVisualization.html",
            "date": " ‚Ä¢ Oct 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "LSTM Algorithm & Implementation",
            "content": "Algorithm . Long-term-short-memory . . . TIme Series Analysis . purpose . Description | Modeling .. inference model and grasp feature time series | Prediction | Control .. manipulate variables to fluctuate purpose variable | TIme Series Analysis in RNN . RNN dose&#39;nt require prior knowledge for modeling trends &amp; period is learned from data autmatically | . | RNN weaksness when prediction of stock if there are only 3 years data, RNN can&#39;t perform well | . | . . Compare for methods in Time Series Forecasting . RNN(LSTM) is more expressionable in terms of function (but Blackbox) . Implementaion (pytorch) . Model Define . import torch import torch.nn as nn . input_dim = 5 # e.g. input of dimension 5 will look like this [1, 3, 8, 2, 3] hidden_dim = 10 # if the hidden dimension is 3 [3, 5, 4] n_layers = 1 # the number of LSTM layers stacked lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True) . Input Data . batch_size = 1 seq_len = 1 # initialize input, hidden state, cell state(long_memory) inp = torch.randn(batch_size, seq_len, input_dim) hidden_state = torch.randn(n_layers, batch_size, hidden_dim) cell_state = torch.randn(n_layers, batch_size, hidden_dim) hidden = (hidden_state, cell_state) print(&quot;inp_size: {}, n hidden_state_size: {}, n cell_state_size: {}&quot;.format(inp.shape, hidden_state.shape, cell_state.shape)) hidden . inp_size: torch.Size([1, 1, 5]), hidden_state_size: torch.Size([1, 1, 10]), cell_state_size: torch.Size([1, 1, 10]) . (tensor([[[ 2.7525, -0.3706, -0.1003, -0.8155, 0.1962, 1.3871, 0.4509, 0.3741, 0.4785, -0.6127]]]), tensor([[[ 1.1783, -0.6752, -1.6241, 0.0391, -1.1110, -0.5538, -1.5670, 0.1072, 0.0810, -0.4173]]])) . out, hidden = lstm_layer(inp, hidden) print(&quot;Outout shape:&quot;, out.shape) print(&quot;Hiden&quot;, hidden) . Outout shape: torch.Size([1, 1, 10]) Hiden (tensor([[[ 0.1804, -0.1010, -0.3767, -0.0170, -0.0909, 0.0929, -0.4508, 0.0109, 0.1320, -0.1107]]], grad_fn=&lt;StackBackward&gt;), tensor([[[ 0.2725, -0.1367, -0.8260, -0.0767, -0.1377, 0.2064, -1.0127, 0.0231, 0.2400, -0.1765]]], grad_fn=&lt;StackBackward&gt;)) . .. LSTM cell process the input and hidden states at each time step . seq_len = 3 inp = torch.randn(batch_size, seq_len, input_dim) out, hidden = lstm_layer(inp, hidden) print(out.shape) . torch.Size([1, 3, 10]) . .. the output&#39;s 2nd dimension is 3, indicating that there were 3 outputs given by the LSTM. This corresponds to the length of our input sequence. . Reference . viya-recurrent-neural-network.pdf | Long Short-Term Memory: From Zero to Hero with PyTorch | .",
            "url": "https://interkid.github.io/My_Research/2021/10/09/LSTM_introduction.html",
            "relUrl": "/2021/10/09/LSTM_introduction.html",
            "date": " ‚Ä¢ Oct 9, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "KNN Algorithm",
            "content": "KNN: What&#39;s it for . simple classification or regeression algorithm simple = low accuracy rather than other Algorithm | . | Regression Find the K closest points to a sample point and return the average value | . | . . The KNN&#8217;s steps are: . Receive an unclassified data; | Measure the distance (Euclidian, Manhattan, Minkowski or Weighted) from the new data to all others data that is already classified; | Gets the K(K is a parameter that you difine) smaller distances; | Check the list of classes had the shortest distance and count the amount of each class that appears; | Takes as correct class the class that appeared the most times; | Classifies the new data with the class that you took in step 5; | How is it used? . Dimensionality reduction stage Avoid sparse data | . | . Implementation Iris dataset . import numpy as np import matplotlib.pyplot as plt import pandas as pd . Data . url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot; names = [&#39;sepal-length&#39;, &#39;sepal-width&#39;, &#39;petal-length&#39;, &#39;petal-width&#39;, &#39;Class&#39;] dataset = pd.read_csv(url, names=names) . dataset . sepal-length sepal-width petal-length petal-width Class . 0 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . ... ... | ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | Iris-virginica | . 146 6.3 | 2.5 | 5.0 | 1.9 | Iris-virginica | . 147 6.5 | 3.0 | 5.2 | 2.0 | Iris-virginica | . 148 6.2 | 3.4 | 5.4 | 2.3 | Iris-virginica | . 149 5.9 | 3.0 | 5.1 | 1.8 | Iris-virginica | . 150 rows √ó 5 columns . X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values # Train Test Split from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # Feature Scaling from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) . KNN . from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors=5) classifier.fit(X_train, y_train) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . y_pred = classifier.predict(X_test) # Evaluating from sklearn.metrics import classification_report, confusion_matrix print(classification_report(y_test, y_pred)) . precision recall f1-score support Iris-setosa 1.00 1.00 1.00 8 Iris-versicolor 0.93 0.88 0.90 16 Iris-virginica 0.71 0.83 0.77 6 accuracy 0.90 30 macro avg 0.88 0.90 0.89 30 weighted avg 0.91 0.90 0.90 30 . K-means algorithm . K-means: What&#39;s it for . Unsupervised Learning for clustering | Non-hierarchical cluster / hierarchical cluster | . . The K-means steps are: . Select the value of K, to decide the number of clusters to be formed. | Select random K points which will act as centroids | Assign each data point, based on their distance from the randomly selected points (Centroid), to the nearest/closest centroid which will form the predefined clusters. | place a new centroid of each cluster. | Repeat step no.3 | If any reassignment occurs, then go to step-4 else go to Step 7. | Finish | Set randomly centeroid in step2, so It is important first value . to use effectively setting automatically initial centroid | . Reference . KNN (K-Nearest Neighbors) #1 | K Means Clustering Simplified in Python | K-Nearest Neighbors Algorithm in Python and Scikit-Learn | .",
            "url": "https://interkid.github.io/My_Research/2021/10/05/KNN.html",
            "relUrl": "/2021/10/05/KNN.html",
            "date": " ‚Ä¢ Oct 5, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://interkid.github.io/My_Research/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://interkid.github.io/My_Research/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . I am Kaito Honda software Deveoper in Japan.I am launching my career in data science. . I am lover of Science / Math. . . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://interkid.github.io/My_Research/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://interkid.github.io/My_Research/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}