{
  
    
        "post0": {
            "title": "Title",
            "content": "Reccomend system introduction . Goal . from interset users show, making methods for recommendation that user would have interset. (ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç¤ºã—ã¦ã„ã‚‹ç¾åœ¨ã®èˆˆå‘³ã‹ã‚‰ã€æ–°ã—ãèˆˆå‘³ã‚’æŒã¤ã§ã‚ã‚ã†åˆ†é‡ã‚’ãŠå‹§ã‚ã™ã‚‹æ–¹æ³•ã‚’ä½œã‚‹) . methods . reccomend ranking deceinding oreder (this aritcle don&#39;t treat) | user based collaborative filtering(ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°) | item based collaborative filtering(ã‚¢ã‚¤ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°) | . . user based collaborative filtering . policy . search user who is silimilar to target user | suggest interest that group of users has to target user | . how to . use cosaine similarity (0: not same interst / 1 same interest ) | . users Data . users_interests = [ [&quot;Hadoop&quot;, &quot;Big Data&quot;, &quot;HBase&quot;, &quot;Java&quot;, &quot;Spark&quot;, &quot;Storm&quot;, &quot;Cassandra&quot;], [&quot;NoSQL&quot;, &quot;MongoDB&quot;, &quot;Cassandra&quot;, &quot;HBase&quot;, &quot;Postgres&quot;], [&quot;Python&quot;, &quot;scikit-learn&quot;, &quot;scipy&quot;, &quot;numpy&quot;, &quot;statsmodels&quot;, &quot;pandas&quot;], [&quot;R&quot;, &quot;Python&quot;, &quot;statistics&quot;, &quot;regression&quot;, &quot;probability&quot;], [&quot;machine learning&quot;, &quot;regression&quot;, &quot;decision trees&quot;, &quot;libsvm&quot;], [&quot;Python&quot;, &quot;R&quot;, &quot;Java&quot;, &quot;C++&quot;, &quot;Haskell&quot;, &quot;programming languages&quot;], [&quot;statistics&quot;, &quot;probability&quot;, &quot;mathematics&quot;, &quot;theory&quot;], [&quot;machine learning&quot;, &quot;scikit-learn&quot;, &quot;Mahout&quot;, &quot;neural networks&quot;], [&quot;neural networks&quot;, &quot;deep learning&quot;, &quot;Big Data&quot;, &quot;artificial intelligence&quot;], [&quot;Hadoop&quot;, &quot;Java&quot;, &quot;MapReduce&quot;, &quot;Big Data&quot;], [&quot;statistics&quot;, &quot;R&quot;, &quot;statsmodels&quot;], [&quot;C++&quot;, &quot;deep learning&quot;, &quot;artificial intelligence&quot;, &quot;probability&quot;], [&quot;pandas&quot;, &quot;R&quot;, &quot;Python&quot;], [&quot;databases&quot;, &quot;HBase&quot;, &quot;Postgres&quot;, &quot;MySQL&quot;, &quot;MongoDB&quot;], [&quot;libsvm&quot;, &quot;regression&quot;, &quot;support vector machines&quot;] ] . unique_interests = sorted({interest for user_interests in users_interests for interest in user_interests}) unique_interests . [&#39;Big Data&#39;, &#39;C++&#39;, &#39;Cassandra&#39;, &#39;HBase&#39;, &#39;Hadoop&#39;, &#39;Haskell&#39;, &#39;Java&#39;, &#39;Mahout&#39;, &#39;MapReduce&#39;, &#39;MongoDB&#39;, &#39;MySQL&#39;, &#39;NoSQL&#39;, &#39;Postgres&#39;, &#39;Python&#39;, &#39;R&#39;, &#39;Spark&#39;, &#39;Storm&#39;, &#39;artificial intelligence&#39;, &#39;databases&#39;, &#39;decision trees&#39;, &#39;deep learning&#39;, &#39;libsvm&#39;, &#39;machine learning&#39;, &#39;mathematics&#39;, &#39;neural networks&#39;, &#39;numpy&#39;, &#39;pandas&#39;, &#39;probability&#39;, &#39;programming languages&#39;, &#39;regression&#39;, &#39;scikit-learn&#39;, &#39;scipy&#39;, &#39;statistics&#39;, &#39;statsmodels&#39;, &#39;support vector machines&#39;, &#39;theory&#39;] . def make_user_interest_vector(user_interests): &quot;&quot;&quot; Given a list ofinterests, produce a vector whose its element is 1 if unique_interests[i] is in the list, 0 otherwise &quot;&quot;&quot; return [1 if interest in user_interests else 0 for interest in unique_interests] . user_interest_vectors = [make_user_interest_vector(user_interests) for user_interests in users_interests] len(user_interest_vectors) user_interest_vectors . [[1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]] . Utils . import math def dot(v, w): &quot;&quot;&quot;Computes v_1 * w_1 + ... + v_n * w_n&quot;&quot;&quot; assert len(v) == len(w), &quot;vectors must be same length&quot; return sum(v_i * w_i for v_i, w_i in zip(v, w)) def cosine_similarity(v1, v2): return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2)) . user_similarities = [[ cosine_similarity(interest_vector_i, interest_vector_j) for interest_vector_j in user_interest_vectors] for interest_vector_i in user_interest_vectors] print(len(user_similarities)) print(user_similarities[0]) # similarity (user0&amp;user1) print(user_similarities[0][1]) print(user_similarities[1][0]) . 15 [1.0, 0.3380617018914066, 0.0, 0.0, 0.0, 0.1543033499620919, 0.0, 0.0, 0.1889822365046136, 0.5669467095138409, 0.0, 0.0, 0.0, 0.1690308509457033, 0.0] 0.3380617018914066 0.3380617018914066 . Show user similarity . print(users_interests[0], users_interests[9], user_similarities[0][9]) # user0, 8 share interest only &quot;Big Data&quot; print(users_interests[0], users_interests[8], user_similarities[0][8]) . [&#39;Hadoop&#39;, &#39;Big Data&#39;, &#39;HBase&#39;, &#39;Java&#39;, &#39;Spark&#39;, &#39;Storm&#39;, &#39;Cassandra&#39;] [&#39;Hadoop&#39;, &#39;Java&#39;, &#39;MapReduce&#39;, &#39;Big Data&#39;] 0.5669467095138409 [&#39;Hadoop&#39;, &#39;Big Data&#39;, &#39;HBase&#39;, &#39;Java&#39;, &#39;Spark&#39;, &#39;Storm&#39;, &#39;Cassandra&#39;] [&#39;neural networks&#39;, &#39;deep learning&#39;, &#39;Big Data&#39;, &#39;artificial intelligence&#39;] 0.1889822365046136 . def most_similar_users_to(user_id): # find other users with nonzero similality pairs = [(other_user_id, similarity) for other_user_id, similarity in enumerate(user_similarities[user_id]) if user_id != other_user_id and similarity &gt; 0] # sort them most similar first return sorted(pairs, key = lambda pair: pair[-1], reverse = True) . most_similar_users_to(0) . [(9, 0.5669467095138409), (1, 0.3380617018914066), (8, 0.1889822365046136), (13, 0.1690308509457033), (5, 0.1543033499620919)] . user0 is similar to 9. cosaine similarity is about 0.56 . suggest recomendation . sum up the similarities every each domain . from collections import defaultdict def user_based_suggestions(user_id, include_current_interests=False): # Sum up the similarities. suggestions: Dict[str, float] = defaultdict(float) for other_user_id, similarity in most_similar_users_to(user_id): for interest in users_interests[other_user_id]: suggestions[interest] += similarity # Convert them to a sorted list. suggestions = sorted(suggestions.items(), key=lambda pair: pair[-1], # weight reverse=True) # And (maybe) exclude already-interests if include_current_interests: return suggestions else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] . ubs0 = user_based_suggestions(0) ubs0 . [(&#39;MapReduce&#39;, 0.5669467095138409), (&#39;MongoDB&#39;, 0.50709255283711), (&#39;Postgres&#39;, 0.50709255283711), (&#39;NoSQL&#39;, 0.3380617018914066), (&#39;neural networks&#39;, 0.1889822365046136), (&#39;deep learning&#39;, 0.1889822365046136), (&#39;artificial intelligence&#39;, 0.1889822365046136), (&#39;databases&#39;, 0.1690308509457033), (&#39;MySQL&#39;, 0.1690308509457033), (&#39;Python&#39;, 0.1543033499620919), (&#39;R&#39;, 0.1543033499620919), (&#39;C++&#39;, 0.1543033499620919), (&#39;Haskell&#39;, 0.1543033499620919), (&#39;programming languages&#39;, 0.1543033499620919)] . Weak point user based collaborative filtering . if number of elements increase , then don&#39;t sugget well for curse of dimensionality ğŸ‘‰ then content base try to solve its problem | . | . item based collaborative filtering . unique_interests[0] . &#39;Big Data&#39; . interest_user_matrix = [[user_interest_vector[j] for user_interest_vector in user_interest_vectors] for j, _ in enumerate(unique_interests)] . print(unique_interests[0]) # ex) user 0, 8, 9 has interest &quot;Big Data&quot; interest_user_matrix . Big Data . [[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]] . if same users are interested in 2 items , cosaine similarity is 1, if not similarity is 0 . interest_similarities = [[cosine_similarity(user_vector_i, user_vector_j) for user_vector_j in interest_user_matrix] for user_vector_i in interest_user_matrix] . interest_similarities[0] . [1.0, 0.0, 0.4082482904638631, 0.3333333333333333, 0.8164965809277261, 0.0, 0.6666666666666666, 0.0, 0.5773502691896258, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896258, 0.5773502691896258, 0.4082482904638631, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.4082482904638631, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] . def most_similar_interests_to(interest_id): similarities = interest_similarities[interest_id] pairs = [(unique_interests[other_interest_id], similarity) for other_interest_id, similarity in enumerate(similarities) if interest_id != other_interest_id and similarity &gt; 0] return sorted(pairs, key=lambda pair: pair[-1], reverse=True) . for example, similarity with &quot;Big Data&quot; is below . msit0 = most_similar_interests_to(0) msit0 . [(&#39;Hadoop&#39;, 0.8164965809277261), (&#39;Java&#39;, 0.6666666666666666), (&#39;MapReduce&#39;, 0.5773502691896258), (&#39;Spark&#39;, 0.5773502691896258), (&#39;Storm&#39;, 0.5773502691896258), (&#39;Cassandra&#39;, 0.4082482904638631), (&#39;artificial intelligence&#39;, 0.4082482904638631), (&#39;deep learning&#39;, 0.4082482904638631), (&#39;neural networks&#39;, 0.4082482904638631), (&#39;HBase&#39;, 0.3333333333333333)] . for interest, similarity in most_similar_interests_to(0): print(interest, similarity) . Hadoop 0.8164965809277261 Java 0.6666666666666666 MapReduce 0.5773502691896258 Spark 0.5773502691896258 Storm 0.5773502691896258 Cassandra 0.4082482904638631 artificial intelligence 0.4082482904638631 deep learning 0.4082482904638631 neural networks 0.4082482904638631 HBase 0.3333333333333333 . def item_based_suggestions(user_id, include_current_interests=False): # Add up the similar interests suggestions = defaultdict(float) user_interest_vector = user_interest_vectors[user_id] for interest_id, is_interested in enumerate(user_interest_vector): similar_interests = most_similar_interests_to(interest_id) for interest, similarity in similar_interests: suggestions[interest] += similarity # Sort them by weight suggestions = sorted(suggestions.items(), key=lambda pair: pair[-1], reverse=True) print(&quot;suggestion is : {}&quot;.format(suggestions)) # And (maybe) exclude already-interests if include_current_interests: return suggestion else: return [(suggestion, weight) for suggestion, weight in suggestions if suggestion not in users_interests[user_id]] . recommendation to user0 . ibs0 = item_based_suggestions(0) ibs0 . suggestion is : [(&#39;HBase&#39;, 6.411156045861838), (&#39;Java&#39;, 6.097094776993208), (&#39;Python&#39;, 5.672467491345621), (&#39;Cassandra&#39;, 5.2543135054150945), (&#39;Big Data&#39;, 5.1815405503520555), (&#39;Hadoop&#39;, 4.662561795878958), (&#39;scikit-learn&#39;, 4.474873734152916), (&#39;MongoDB&#39;, 4.437816924487368), (&#39;Postgres&#39;, 4.437816924487368), (&#39;R&#39;, 4.2540358447538855), (&#39;Spark&#39;, 4.146264369941973), (&#39;Storm&#39;, 4.146264369941973), (&#39;probability&#39;, 3.9567956789604666), (&#39;C++&#39;, 3.9378169244873686), (&#39;regression&#39;, 3.6234623456271335), (&#39;numpy&#39;, 3.6213203435596424), (&#39;scipy&#39;, 3.6213203435596424), (&#39;statsmodels&#39;, 3.5295686340235055), (&#39;pandas&#39;, 3.4748737341529163), (&#39;statistics&#39;, 3.4289742326275534), (&#39;machine learning&#39;, 3.322461852836958), (&#39;Haskell&#39;, 3.284457050376173), (&#39;programming languages&#39;, 3.284457050376173), (&#39;neural networks&#39;, 3.1153550716504106), (&#39;MySQL&#39;, 2.9915638315627207), (&#39;databases&#39;, 2.9915638315627207), (&#39;artificial intelligence&#39;, 2.8164965809277263), (&#39;deep learning&#39;, 2.8164965809277263), (&#39;libsvm&#39;, 2.730710143300821), (&#39;NoSQL&#39;, 2.698670612749268), (&#39;theory&#39;, 2.1547005383792515), (&#39;mathematics&#39;, 2.1547005383792515), (&#39;Mahout&#39;, 2.1213203435596424), (&#39;decision trees&#39;, 1.9915638315627207), (&#39;MapReduce&#39;, 1.861807319565799), (&#39;support vector machines&#39;, 1.2844570503761732)] . [(&#39;Python&#39;, 5.672467491345621), (&#39;scikit-learn&#39;, 4.474873734152916), (&#39;MongoDB&#39;, 4.437816924487368), (&#39;Postgres&#39;, 4.437816924487368), (&#39;R&#39;, 4.2540358447538855), (&#39;probability&#39;, 3.9567956789604666), (&#39;C++&#39;, 3.9378169244873686), (&#39;regression&#39;, 3.6234623456271335), (&#39;numpy&#39;, 3.6213203435596424), (&#39;scipy&#39;, 3.6213203435596424), (&#39;statsmodels&#39;, 3.5295686340235055), (&#39;pandas&#39;, 3.4748737341529163), (&#39;statistics&#39;, 3.4289742326275534), (&#39;machine learning&#39;, 3.322461852836958), (&#39;Haskell&#39;, 3.284457050376173), (&#39;programming languages&#39;, 3.284457050376173), (&#39;neural networks&#39;, 3.1153550716504106), (&#39;MySQL&#39;, 2.9915638315627207), (&#39;databases&#39;, 2.9915638315627207), (&#39;artificial intelligence&#39;, 2.8164965809277263), (&#39;deep learning&#39;, 2.8164965809277263), (&#39;libsvm&#39;, 2.730710143300821), (&#39;NoSQL&#39;, 2.698670612749268), (&#39;theory&#39;, 2.1547005383792515), (&#39;mathematics&#39;, 2.1547005383792515), (&#39;Mahout&#39;, 2.1213203435596424), (&#39;decision trees&#39;, 1.9915638315627207), (&#39;MapReduce&#39;, 1.861807319565799), (&#39;support vector machines&#39;, 1.2844570503761732)] . REF . ç¬¬3å›ã€€ãƒ™ã‚¤ã‚¸ã‚¢ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ã‚’å®Ÿè£…ã—ã¦ã¿ã‚ˆã† | data-science-from-scratch | .",
            "url": "https://interkid.github.io/My_Research/2021/10/30/RecommendSystem.html",
            "relUrl": "/2021/10/30/RecommendSystem.html",
            "date": " â€¢ Oct 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Math for Data Science",
            "content": "!pip install d2l !pip install -U pandas_profiling import os os.kill(os.getpid(), 9) . Collecting d2l Downloading d2l-0.17.0-py3-none-any.whl (83 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83 kB 1.5 MB/s Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5) Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2) Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.3.1) Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.6.1) Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.1.1) Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (5.2.0) Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (7.6.5) Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter-&gt;d2l) (4.10.1) Requirement already satisfied: tornado&gt;=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.1.1) Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.3.5) Requirement already satisfied: ipython&gt;=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.5.0) Requirement already satisfied: traitlets&gt;=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel-&gt;jupyter-&gt;d2l) (5.1.0) Requirement already satisfied: simplegeneric&gt;0.8 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.8.1) Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.7.5) Requirement already satisfied: prompt-toolkit&lt;2.0.0,&gt;=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (1.0.18) Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (4.8.0) Requirement already satisfied: setuptools&gt;=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (57.4.0) Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (4.4.2) Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (2.6.1) Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (0.2.5) Requirement already satisfied: six&gt;=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit&lt;2.0.0,&gt;=1.0.4-&gt;ipython&gt;=4.0.0-&gt;ipykernel-&gt;jupyter-&gt;d2l) (1.15.0) Requirement already satisfied: jupyterlab-widgets&gt;=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (1.0.2) Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (0.2.0) Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (3.5.1) Requirement already satisfied: nbformat&gt;=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets-&gt;jupyter-&gt;d2l) (5.1.3) Requirement already satisfied: jsonschema!=2.5.0,&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;jupyter-&gt;d2l) (2.6.0) Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat&gt;=4.2.0-&gt;ipywidgets-&gt;jupyter-&gt;d2l) (4.8.1) Requirement already satisfied: terminado&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (0.12.1) Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (1.8.0) Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook-&gt;jupyter-&gt;d2l) (2.11.3) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel-&gt;jupyter-&gt;d2l) (2.8.2) Requirement already satisfied: pyzmq&gt;=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client-&gt;ipykernel-&gt;jupyter-&gt;d2l) (22.3.0) Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado&gt;=0.8.1-&gt;notebook-&gt;jupyter-&gt;d2l) (0.7.0) Requirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-&gt;notebook-&gt;jupyter-&gt;d2l) (2.0.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (0.10.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;d2l) (2.4.7) Requirement already satisfied: mistune&lt;2,&gt;=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.8.4) Requirement already satisfied: entrypoints&gt;=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.3) Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.5.0) Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (4.1.0) Requirement already satisfied: pandocfilters&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (1.5.0) Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert-&gt;jupyter-&gt;d2l) (0.7.1) Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;d2l) (21.0) Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach-&gt;nbconvert-&gt;jupyter-&gt;d2l) (0.5.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;d2l) (2018.9) Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole-&gt;jupyter-&gt;d2l) (1.11.2) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (1.24.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (2.10) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;d2l) (2021.5.30) Installing collected packages: d2l Successfully installed d2l-0.17.0 Requirement already satisfied: pandas_profiling in /usr/local/lib/python3.7/dist-packages (1.4.1) Collecting pandas_profiling Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 261 kB 5.4 MB/s Collecting multimethod&gt;=1.4 Downloading multimethod-1.6-py3-none-any.whl (9.4 kB) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.19.5) Requirement already satisfied: seaborn&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (0.11.2) Requirement already satisfied: jinja2&gt;=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (2.11.3) Requirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (2.0.1) Collecting visions[type_image_path]==0.7.4 Downloading visions-0.7.4-py3-none-any.whl (102 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102 kB 12.8 MB/s Collecting PyYAML&gt;=5.0.0 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 49.3 MB/s Requirement already satisfied: scipy&gt;=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.4.1) Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.1.5) Collecting htmlmin&gt;=0.1.12 Downloading htmlmin-0.1.12.tar.gz (19 kB) Requirement already satisfied: tqdm&gt;=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (4.62.3) Requirement already satisfied: matplotlib&gt;=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (3.2.2) Collecting phik&gt;=0.11.1 Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 675 kB 46.4 MB/s Requirement already satisfied: missingno&gt;=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (0.5.0) Collecting tangled-up-in-unicode==0.1.0 Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1 MB 56.0 MB/s Collecting pydantic&gt;=1.8.1 Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.1 MB 77.4 MB/s Requirement already satisfied: joblib~=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas_profiling) (1.0.1) Collecting requests&gt;=2.24.0 Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62 kB 849 kB/s Requirement already satisfied: networkx&gt;=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas_profiling) (2.6.3) Requirement already satisfied: attrs&gt;=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas_profiling) (21.2.0) Collecting imagehash Downloading ImageHash-4.2.1.tar.gz (812 kB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812 kB 83.8 MB/s Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4-&gt;pandas_profiling) (7.1.2) Requirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (2.8.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&gt;=3.2.0-&gt;pandas_profiling) (0.10.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=3.2.0-&gt;pandas_profiling) (1.15.0) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,&gt;=0.25.3-&gt;pandas_profiling) (2018.9) Collecting scipy&gt;=1.4.1 Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB) |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.5 MB 49 kB/s Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic&gt;=1.8.1-&gt;pandas_profiling) (3.7.4.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (2021.5.30) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (1.24.3) Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (2.0.6) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.24.0-&gt;pandas_profiling) (2.10) Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash-&gt;visions[type_image_path]==0.7.4-&gt;pandas_profiling) (1.1.1) Building wheels for collected packages: htmlmin, imagehash Building wheel for htmlmin (setup.py) ... done Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=b7c3b2b05d41f3398f13252e866d4fe387bbf6ca410bc70584eac03fa0a9ee4d Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655 Building wheel for imagehash (setup.py) ... done Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295207 sha256=ac7c7cb85efa8bcc38664c9961891f32b4b301615131d775bfefa85f230789a1 Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e Successfully built htmlmin imagehash Installing collected packages: tangled-up-in-unicode, scipy, multimethod, visions, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling Attempting uninstall: scipy Found existing installation: scipy 1.4.1 Uninstalling scipy-1.4.1: Successfully uninstalled scipy-1.4.1 Attempting uninstall: requests Found existing installation: requests 2.23.0 Uninstalling requests-2.23.0: Successfully uninstalled requests-2.23.0 Attempting uninstall: PyYAML Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Attempting uninstall: pandas-profiling Found existing installation: pandas-profiling 1.4.1 Uninstalling pandas-profiling-1.4.1: Successfully uninstalled pandas-profiling-1.4.1 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible. datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible. albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible. Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 multimethod-1.6 pandas-profiling-3.1.0 phik-0.12.0 pydantic-1.8.2 requests-2.26.0 scipy-1.7.1 tangled-up-in-unicode-0.1.0 visions-0.7.4 . Part 1 Theoretical explanation . Differential calculus . The best constant approximation for the rate of change . . $L = lim_{hâ†’0} frac{f(a+h) - f(a)}{h}$ . Power rules $ frac{d}{dx}x^a=ax^{a-1}$ . | Exponential and logarthm $ frac{d}{dx}e^x=e^x$ $ frac{d}{dx}a^x=a^xln(a)$ . | Sum rule $(af+ beta g)` = af` + beta g`$ . | Product rule $(fg)` = f`g +fg`$ . | Quotient rule $( frac{f}{g})` = frac{f`g-fg`}{g^2}$ . | . Integral Calculus . an integral assigns numbers to functions in a way that describes displacement, area, volume, and other concepts that arise by combining infinitesimal data . Application: EM Algorithm . ${ displaystyle L({ boldsymbol { theta }}; mathbf {X} )=p( mathbf {X} mid { boldsymbol { theta }})= int p( mathbf {X} , mathbf {Z} mid { boldsymbol { theta }}) ,d mathbf {Z} = int p( mathbf {Z} mid mathbf {X} ,{ boldsymbol { theta }})p( mathbf {X} mid { boldsymbol { theta }}) ,d mathbf {Z} }$ ${ displaystyle Q({ boldsymbol { theta }} mid { boldsymbol { theta }}^{(t)})= operatorname {E} _{ mathbf {Z} mid mathbf {X} ,{ boldsymbol { theta }}^{(t)}} left[ log L({ boldsymbol { theta }}; mathbf {X} , mathbf {Z} ) right] ,}$ . %matplotlib inline import torch from IPython import display from mpl_toolkits import mplot3d from d2l import torch as d2l x = torch.arange(-2, 2, 0.01) f = torch.exp(-x**2) d2l.set_figsize() d2l.plt.plot(x, f, color=&#39;black&#39;) d2l.plt.fill_between(x.tolist(), f.tolist()) d2l.plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; x = torch.arange(-2, 2, 0.01) f = torch.exp(-x**2) d2l.set_figsize() d2l.plt.plot(x, f, color=&#39;black&#39;) d2l.plt.fill_between(x.tolist()[50:250], f.tolist()[50:250]) d2l.plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; from scipy import stats distribution = &quot;normal&quot; #@param [&quot;normal&quot;, &quot;gamma&quot;, &quot;uniform&quot;, &quot;laplace&quot;, &quot;double weibull&quot;] {allow-input: true} epsilon = 0.25224 #@param {type:&quot;slider&quot;, min:0, max:1, step:0.00001} min = -8 #@param {type:&quot;number&quot;} max = 8#@param {type:&quot;number&quot;} x = torch.arange(min, max, epsilon) #f = x / (1 + x**2) if distribution == &quot;normal&quot;: f=torch.tensor(stats.norm(0, 1).pdf(x)) elif distribution == &quot;gamma&quot;: f = torch.tensor(stats.gamma.pdf(x, 0.6)) elif distribution == &quot;uniform&quot;: f = torch.tensor(stats.uniform.pdf(x)) elif distribution == &quot;laplace&quot;: f = torch.tensor(stats.laplace.pdf(x)) elif distribution == &quot;double weibull&quot;: f = torch.tensor(stats.dweibull.pdf(x, 2.07)) approx = torch.sum(epsilon * f) #true = torch.log(torch.tensor([5.])) / 2 d2l.set_figsize() d2l.plt.bar(x, f, width=epsilon, align=&#39;edge&#39;) d2l.plt.plot(x, f, color=&#39;black&#39;) d2l.plt.show() f&#39;approximation: {approx}&#39; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; &#39;approximation: 1.0000000130791062&#39; . Gradient Descent Algorithm . . . Gradient Descent minimize the loss between objective and cost function by taking partial derivatives with respect to each parameter(w, b in this example) . ãƒ»Gradient $ nabla L = ( frac{ partial L}{ partial w} frac{ partial L}{ partial b})$ . ãƒ»Partial Derivatives $ frac{ partial}{ partial w}((y-(wx^2+b)) = -2x^2(-b-wx^2+y)$ $ frac{ partial}{ partial b}((y-(wx^2+b)) = -2(-b-wx^2+y)$ . ãƒ»Update parameters $w â† w - eta frac{ partial f}{ partial w} $ $b â† b - eta frac{ partial f}{ partial b} $ . Neural Network . Reference . Math for Data Science - Online Workshop | .",
            "url": "https://interkid.github.io/My_Research/2021/10/27/MathNotebook.html",
            "relUrl": "/2021/10/27/MathNotebook.html",
            "date": " â€¢ Oct 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "install surf2stl-python . !git clone https://github.com/asahidari/surf2stl-python . Cloning into &#39;surf2stl-python&#39;... remote: Enumerating objects: 19, done. remote: Counting objects: 100% (19/19), done. remote: Compressing objects: 100% (16/16), done. remote: Total 19 (delta 4), reused 17 (delta 2), pack-reused 0 Unpacking objects: 100% (19/19), done. . cd surf2stl-python . /content/surf2stl-python . How to draw CalbiYau Manifold . $$ z^n_1 + z^n_2 = 1 $$$$ z_1= e^{iÏ†}[cos(x + iy)]^ frac2{n} $$$$ z_2= e^{iÏ†}[sin (x + iy)]^ frac2{n} $$$$ Ï†_1= frac{2Ï€k_1}{n} (0 â‰¦ k &lt; n) $$$$ Ï†_2= frac{2Ï€k_1}{n} (0 â‰¦ k &lt; n) $$ . Parameter k1 and k2 individually take Integer values from 0 to n - 1, and results in n x n parts of the manifold(manupulate x, y each pattern in $nxn=n^2$) . | to visualize Calabi-Yau manifold means that to satisfy equation $z^n_1 + z^n_2 = 1$ ,then we can get z1, z2 by moving parameter x,y and integer k1, k2 . | $z_1, z_2$ spread 4Dimention thinking considering real number&amp;imaginary number($Re(z_1),Im(z_1),Re(z_2),Im(z_2)$ ), so we should think to reduce dimention | reduce $Im(z_1),Im(z_2)$ then make $(Re(z1),Re(z2),Im(z1)cos(a)+Im(z2)sin(a))$ 3D | . Import library . import numpy as np import math, cmath # cmath: è¤‡ç´ æ•°ã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D import matplotlib.ticker as plticker from matplotlib import cm from scipy.spatial import Delaunay import surf2stl as s2s . Set Parameter . N = 9 # Dimension a = 0.4 row, col = 30, 30 writeSTL = False . define function for caculation . def calcZ1(x, y, k, n): return cmath.exp(1j*(2*cmath.pi*k/n)) * (cmath.cos(x+y*1j)**(2/n)) def calcZ2(x, y, k, n): return cmath.exp(1j*(2*cmath.pi*k/n)) * (cmath.sin(x+y*1j)**(2/n)) def calcZ1Real(x, y, k, n): return (calcZ1(x, y, k, n)).real def calcZ2Real(x, y, k, n): return (calcZ2(x, y, k, n)).real def calcZ(x, y, k1_, k2_, n, a_): z1 = calcZ1(x, y, k1, n) z2 = calcZ2(x, y, k2, n) return z1.imag * math.cos(a_) + z2.imag*math.sin(a_) . Draw . x = np.linspace(0, math.pi/2, col) y = np.linspace(-math.pi/2, math.pi/2, row) x, y = np.meshgrid(x, y) # init graph fig = plt.figure(figsize=(18,8)) for n in range(2, N): ax = fig.add_subplot(2, 4, n - 1, projection=&#39;3d&#39;) ax.view_init(elev=15, azim=15) ax.set_title(&quot;n=%d&quot; % n) ax.set_xlabel(&#39;X&#39;) ax.set_ylabel(&#39;Y&#39;) loc = plticker.MultipleLocator(base=1.0) # this locator puts ticks at regular intervals ax.xaxis.set_major_locator(loc) ax.yaxis.set_major_locator(loc) ax.zaxis.set_major_locator(loc) count = 0 for k1 in range(n): for k2 in range(n): # calc X, Y, Z values X = np.frompyfunc(calcZ1Real, 4, 1)(x, y, k1, n).astype(&#39;float32&#39;) Y = np.frompyfunc(calcZ2Real, 4, 1)(x, y, k2, n).astype(&#39;float32&#39;) Z = np.frompyfunc(calcZ, 6, 1)(x, y, k1, k2, n, a).astype(&#39;float32&#39;) ax.plot_surface(X, Y, Z, cmap=cm.ocean, linewidth=0) . Reference . Creating Calabi Yau Manifold in python | Calabi-Yauå¤šæ§˜ä½“ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã«å¯è¦–åŒ–ã™ã‚‹ | .",
            "url": "https://interkid.github.io/My_Research/2021/10/27/CalabiYauManifold.html",
            "relUrl": "/2021/10/27/CalabiYauManifold.html",
            "date": " â€¢ Oct 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "NewworkXã«ã‚ˆã‚‹å¯è¦–åŒ–/åˆ†æ",
            "content": "&#30446;&#27425; . åŸºæœ¬ | å¯†åº¦ã¨ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•° | ã‚¹ãƒ¢ãƒ¼ãƒ«ãƒ¯ãƒ¼ãƒ«ãƒ‰ç”Ÿæˆ | æ¬¡æ•°ã¨æ¬¡æ•°åˆ†å¸ƒ | ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ãƒªãƒ¼ä½œæˆ | è·é›¢ã¨ç›´å¾„ | ä¸­å¿ƒæ€§ | . NetworkXã§ã§ãã‚‹ã“ã¨ . ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ä½œæˆã®ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ç°¡å˜ãªç”Ÿæˆ ã‚°ãƒ©ãƒ•ã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã¯ã‚·ãƒ³ãƒ—ãƒ«(ç¶ºéº—ãªãƒ¬ãƒ™ãƒ«ã§ã¯ãªã„) | . | ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã®æ–‡è„ˆã§ã®ç”¨é€”ãŒå¤šã„ ex. ä¸­å¿ƒäººç‰©(ãƒãƒ–)ã‚’è¦‹ã¤ã‘ã¦æ„ŸæŸ“åˆ¶å¾¡, ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æŠ½å‡ºã—ã¦ã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  ãªã© | . | . ã“ã®è¨˜äº‹ã®å‚è€ƒã¯ã»ã¼ã“ã¡ã‚‰å¼•ç”¨ğŸ™ https://www.youtube.com/watch?v=AC4u1PYlveU | å…¬å¼ref https://networkx.org/documentation/stable/reference/index.html | . &#22522;&#26412; . import networkx as nx # NetworkXã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ import matplotlib.pyplot as plt import numpy as np . G = nx.Graph([(1, 2), (2, 3), (3, 1)]) # G = nx.Graph() # ç©ºã®ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹å ´åˆ nx.draw(G, with_labels=True,font_family=&#39;sans-serif&#39;) # ãƒ©ãƒ™ãƒ«ã‚’Trueã«ã—ã¦ç•ªå·ã®å¯è¦–åŒ– . æ•°å­—ä»¥å¤–ã®æ—¥æœ¬èª,æ–‡å­—ã®è¨˜è¿°ã‚‚è¨­å®šã‚’åŠ ãˆã‚Œã°å¯èƒ½(defaultã§æ–‡å­—åŒ–ã‘ã™ã‚‹) | . G.add_node(4) # 4ã¯ç¹‹ãŒã£ã¦ãªã„ã®ã§é ãã«é…ç½® nx.draw(G, with_labels=True) . G.add_edge(1,4) nx.draw(G, with_labels=True) . n = [5, 6, 7] G.add_nodes_from(n) # è¤‡æ•°ãƒªãƒ³ã‚¯ã®è¿½åŠ  e = [(5,6), (3,7), (4,6)] G.add_edges_from(e) nx.draw(G, with_labels=True) . G.remove_node(4) nx.draw(G, with_labels=True) . G.remove_edges_from([(1,2), (2,3)]) nx.draw(G, with_labels=True) . rnd = nx.gnp_random_graph(10, 0.5) # (ç¬¬1å¼•æ•°ã€€ãƒãƒ¼ãƒ‰ã®æ•°, ç¬¬ï¼’å¼•æ•°ã€€ã‚¨ãƒƒã‚¸ã‚’å¼•ãç¢ºç‡(1ã§å…¨ã¦å¼•ã)) pos=nx.circular_layout(rnd) # å††å‘¨ä¸Šã«ç­‰é–“éš”ã§é…ç½® #pos = nx.spring_layout(rnd) # ãƒãƒãƒ¢ãƒ‡ãƒ«(ç¹‹ãŒã‚Š(ãƒãƒ)ãŒè‡ªç„¶ã®é•·ã•) #pos = nx.random_layout(rnd) #ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ plt.title(&#39;random graph&#39;) nx.draw(rnd, pos, with_labels=True) . K_10 = nx.complete_graph(10) pos=nx.spring_layout(K_10) plt.title(&#39;complete graph&#39;) nx.draw(K_10, pos, with_labels=True) . star = nx.star_graph(100) pos=nx.spring_layout(star) plt.title(&#39;star graph&#39;) nx.draw(star, pos, with_labels=True) . wheel = nx.wheel_graph(10) pos=nx.spring_layout(wheel) plt.title(&#39;wheel graph&#39;) nx.draw(wheel, pos, with_labels=True) . &#23494;&#24230;&#12392;&#12463;&#12521;&#12473;&#12479;&#20418;&#25968; . å¯†åº¦: ã©ã‚Œã ã‘ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå¯†é›†ã—ã¦ã„ã‚‹ã‹ . å¯†åº¦ã‚’æ±‚ã‚ã‚‹å¼ $$ density = ãƒªãƒ³ã‚¯ã®ç·æ•° / ãƒãƒ¼ãƒ‰ã®ãƒšã‚¢ã®ç·æ•° = frac{m}{n(n-1) -2} $$ . %matplotlib inline import networkx as nx # NetworkXã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ import matplotlib.pyplot as plt import numpy as np # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆ G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (3, 4), (4,5), (4,6),(5,6)]) nx.draw(G, with_labels=True) # ãƒ©ãƒ™ãƒ«ã‚’Trueã«ã—ã¦ç•ªå·ã®å¯è¦–åŒ– . ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•° : éš£æ¥ãƒãƒ¼ãƒ‰åŒå£«ãŒã©ã®ãã‚‰ã„ç¹‹ãŒã£ã¦ã„ã‚‹ã‹ (æ•°å€¤åŒ–ã®æ•°å¼ã¯çœç•¥) . print(nx.average_clustering(G)) . 0.5833333333333334 . %matplotlib inline import networkx as nx # NetworkXã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ import matplotlib.pyplot as plt import numpy as np # ãƒ©ãƒ³ãƒ€ãƒ ã‚°ãƒ©ãƒ•ç”Ÿæˆ rnd = nx.gnp_random_graph(10, 0.1) #pos=nx.spring_layout(rnd) pos=nx.circular_layout(rnd) #pos = nx.random_layout(rnd) plt.title(&#39;random graph&#39;) nx.draw(rnd, pos, with_labels=True) print(&#39;density:&#39;, nx.density(rnd), &#39;, clustering coefficient:&#39;, nx.average_clustering(rnd)) . density: 0.044444444444444446 , clustering coefficient: 0.0 . cycle = nx.cycle_graph(10) pos=nx.spring_layout(cycle) #pos=nx.circular_layout(cycle) #pos=nx.random_layout(cycle) plt.title(&#39;cycle graph&#39;) nx.draw(cycle, pos, with_labels=True) print(&#39;density:&#39;, nx.density(cycle), &#39;, clustering coefficient:&#39;, nx.average_clustering(cycle)) . density: 0.2222222222222222 , clustering coefficient: 0.0 . K_10 = nx.complete_graph(10) pos=nx.spring_layout(K_10) plt.title(&#39;complete graph&#39;) nx.draw(K_10, pos, with_labels=True) print(&#39;density:&#39;, nx.density(K_10), &#39;, clustering coefficient:&#39;, nx.average_clustering(K_10)) . density: 1.0 , clustering coefficient: 1.0 . &#12473;&#12514;&#12540;&#12523;&#12527;&#12540;&#12523;&#12489;&#12493;&#12483;&#12488;&#12527;&#12540;&#12463;&#29983;&#25104; . çµŒè·¯é•·ãŒé•·ã„&amp; é«˜åº¦ã«ã‚¯ãƒ©ã‚¹ã‚¿åŒ–ã‚’å…¼ã­å‚™ãˆãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ . n = 100 # ãƒãƒ¼ãƒ‰æ•° k = 4 # æ¬¡æ•° p = 0.1 # ãƒªãƒ³ã‚¯ã¤ãªãæ›¿ãˆç¢ºç‡ print(&#39;NetworkXã®watts_strogatz_graph()ã‚ˆã‚Šç”Ÿæˆ&#39;) G1 = nx.watts_strogatz_graph(n, k, p) pos = nx.circular_layout(G1) print(nx.info(G1)) # ç¨®ã€…ã®æƒ…å ±ã‚’å‡ºåŠ› print(&#39;ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•°ï¼š&#39;, nx.average_clustering(G1)) # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã®ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•°ã‚’å‡ºåŠ› nx.draw(G1, pos) plt.show() . NetworkXã®watts_strogatz_graph()ã‚ˆã‚Šç”Ÿæˆ Graph with 100 nodes and 200 edges ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•°ï¼š 0.42566666666666664 . def gen_WS_network(n, k, p, seed=None): if seed is not None: np.random.seed(seed=seed) G = nx.Graph() G.add_nodes_from(list(range(n))) for i in range(n): for j in range(k//2): G.add_edge(i, (i+j+1)%n) # k/2æœ¬ãƒªãƒ³ã‚¯ã‚’å¼µã‚‹ for (u,v) in G.edges(): if np.random.rand() &lt; p: # pã®ç¢ºç‡ã§ã¤ãªãæ›¿ãˆ G.remove_edge(u, v) new_node = (u+np.random.randint(n-1)+1)%n # æ–°ã—ã„æ¥ç¶šå…ˆã¨ã—ã¦u+1ã‹ã‚‰u-1ã¾ã§ã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶ while G.has_edge(u, new_node) == True: # æ—¢å­˜ãƒªãƒ³ã‚¯ã¯é™¤å¤– new_node = (u+np.random.randint(n-1)+1)%n G.add_edge(u, new_node) return G . n = 100 k = 4 p = 0.1 print(&#39;è‡ªä½œé–¢æ•°ã«ã‚ˆã‚‹ç”Ÿæˆ&#39;) G2 = gen_WS_network(n, k, p) pos = nx.circular_layout(G2) print(nx.info(G2)) print(&#39;ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•°ï¼š&#39;, nx.average_clustering(G2)) nx.draw(G2, pos) plt.show() . è‡ªä½œé–¢æ•°ã«ã‚ˆã‚‹ç”Ÿæˆ Graph with 100 nodes and 200 edges ã‚¯ãƒ©ã‚¹ã‚¿ä¿‚æ•°ï¼š 0.3556190476190476 . &#27425;&#25968;&#12392;&#27425;&#25968;&#20998;&#24067; . æ¬¡æ•°: ãƒãƒ¼ãƒ‰(é ‚ç‚¹)ãŒæŒã¤ãƒªãƒ³ã‚¯ï¼ˆè¾º)ã®æ•° å¹³å‡æ¬¡æ•°: æ¬¡æ•°kã®å¹³å‡å€¤ æ¬¡æ•°åˆ†å¸ƒ: æ¬¡æ•°ãŒå…¨ãƒãƒ¼ãƒ‰ã«å ã‚ã‚‹å‰²åˆ: p(k) æ£’ã‚°ãƒ©ãƒ•ã§è¡¨ã™ã“ã¨ãŒå¤šã„ . G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (4,5)]) print(nx.info(G)) print(G.nodes()) nx.draw(G, with_labels=True) # ãƒ©ãƒ™ãƒ«ã‚’Trueã«ã—ã¦ç•ªå·ã®å¯è¦–åŒ– . Graph with 5 nodes and 5 edges [1, 2, 3, 4, 5] . print(nx.degree_histogram(G)) # æ™‚æ•°ã®å€‹æ•°ã‚’å‡ºåŠ› degree_dist = [i/5 for i in nx.degree_histogram(G)] plt.rcParams[&#39;font.size&#39;] = 20 plt.bar(range(5), height = degree_dist) # æ™‚æ•°ã®å‰²åˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’å‡ºåŠ› plt.xlabel(&#39;$k$&#39;) plt.ylabel(&#39;$p(k)$&#39;) plt.ylim(0,1) . [0, 2, 2, 0, 1] . (0.0, 1.0) . &#12473;&#12465;&#12540;&#12523;&#12501;&#12522;&#12540;&#12493;&#12483;&#12488;&#12527;&#12540;&#12463; . å¤šæ•°ãŒå°‘æ•°ã¨ã¤ãªãŒã‚Šã€å°‘æ•°ãŒå¤šæ•°ã¨ã¤ãªãŒã£ã¦ã„ã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ãƒªãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(BAãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†) ex WWW, ä¿³å„ªã®å…±æ¼”é–¢ä¿‚ æ™‚æ•°åˆ†å¸ƒã¯ã¹ãå‰‡ã¨ãªã‚‹ å‚è€ƒ: https://syodokukai.exblog.jp/20771928/ . n = 100 m = 4 print(&#39;NetworkX.barabasi_albert_graph()&#39;) G1 = nx.barabasi_albert_graph(n, m) print(nx.info(G1)) nx.draw(G1) plt.show() . NetworkX.barabasi_albert_graph() Graph with 100 nodes and 384 edges . n = 5000 m = 4 # ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ãƒªãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç”Ÿæˆ G = nx.barabasi_albert_graph(n, m) # çµ„ã¿è¾¼ã¿é–¢æ•°ã§ç”Ÿæˆ # G = gen_BA_network(n, m) # è‡ªä½œé–¢æ•°ã§ç”Ÿæˆ k = [i for i,x in enumerate(nx.degree_histogram(G)) if x != 0] degree_dist = [i/n for i in nx.degree_histogram(G) if i != 0] print(k) print(degree_dist) plt.xscale(&#39;log&#39;) plt.yscale(&#39;log&#39;) plt.ylim(0.0001,1) plt.scatter(k, degree_dist) . [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 54, 56, 57, 62, 63, 65, 67, 72, 73, 74, 75, 76, 77, 81, 85, 87, 93, 94, 96, 97, 102, 107, 114, 115, 120, 122, 128, 201, 248, 267] [0.3324, 0.1938, 0.1154, 0.0776, 0.0542, 0.0396, 0.0308, 0.0254, 0.0188, 0.0152, 0.0118, 0.0106, 0.008, 0.007, 0.0056, 0.0048, 0.0052, 0.0034, 0.0038, 0.0024, 0.0028, 0.0014, 0.0018, 0.0012, 0.0022, 0.0016, 0.0018, 0.0012, 0.001, 0.0012, 0.0008, 0.0012, 0.0012, 0.0008, 0.0002, 0.001, 0.0006, 0.0012, 0.0002, 0.0004, 0.0006, 0.0004, 0.0004, 0.0002, 0.0004, 0.0006, 0.0004, 0.0004, 0.0004, 0.0002, 0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0004, 0.0004, 0.0002, 0.0002, 0.0004, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002] . &lt;matplotlib.collections.PathCollection at 0x7f1c8bddf050&gt; . &#36317;&#38626; . è·é›¢: ãƒãƒ¼ãƒ‰ãƒšã‚¢é–“ã®æœ€çŸ­çµŒè·¯ã®é•·ã• | å¹³å‡è·é›¢: å…¨ãƒšã‚¢ã®è·é›¢ã®å¹³å‡å€¤ | ç›´å¾„: å…¨ãƒšã‚¢ã®è·é›¢ã®ä¸­ã§æœ€å¤§ã®ã‚‚ã® | . G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (3, 4), (4,5), (4,6),(5,6)]) nx.draw(G, with_labels=True) # ãƒ©ãƒ™ãƒ«ã‚’Trueã«ã—ã¦ç•ªå·ã®å¯è¦–åŒ– . print(nx.shortest_path_length(G, 1, 2)) print(nx.shortest_path_length(G, 3, 6)) . 1 2 . print(nx.shortest_path(G, 1, 2)) print(nx.shortest_path(G, 2, 6)) print([p for p in nx.all_shortest_paths(G, 2, 6)]) . [1, 2] [2, 1, 4, 6] [[2, 1, 4, 6], [2, 1, 5, 6]] . print(&#39;L=&#39;, nx.average_shortest_path_length(G)) print(&#39;D=&#39;, nx.diameter(G)) . L= 1.5333333333333334 D= 3 . &#20013;&#24515;&#24615;(Centrality) . &quot;ä¸­å¿ƒ&quot;ã®æŒ‡æ¨™ . æ¬¡æ•°ä¸­å¿ƒæ€§: æ¬¡æ•°(ãƒãƒ¼ãƒ‰ãŒæŒã¤ãƒªãƒ³ã‚¯ã®æ•°)ã§æ±ºã‚ã‚‹ / æ­£è¦åŒ–ã—ãŸå€¤ã‚’ç®—å‡º | è¿‘æ¥ä¸­å¿ƒæ€§: ä»–ã®å…¨ã¦ä»–ã®å…¨ã¦ã®ãƒãƒ¼ãƒ‰ã¸ã®è·é›¢(æœ€çŸ­çµŒè·¯é•·)ã®å¹³å‡å€¤ã®é€†æ•° | ä»–ã«ã‚‚æŒ‡æ¨™ã‚ã‚Š | . æ´»ç”¨: snsã®ä¸­å¿ƒæ€§ãªã© . G = nx.Graph([(1, 2), (1,3), (1,4), (1,5), (3, 4), (4,5), (4,6),(5,6)]) nx.draw(G, with_labels=True) # ãƒ©ãƒ™ãƒ«ã‚’Trueã«ã—ã¦ç•ªå·ã®å¯è¦–åŒ– print(&#39;Degree Centrality&#39;, nx.degree_centrality(G)) # æ¬¡æ•°ä¸­å¿ƒæ€§ print(&#39;Closeness Centrality&#39;, nx.closeness_centrality(G)) # è¿‘æ¥ä¸­å¿ƒæ€§ . Degree Centrality {1: 0.8, 2: 0.2, 3: 0.4, 4: 0.8, 5: 0.6000000000000001, 6: 0.4} Closeness Centrality {1: 0.8333333333333334, 2: 0.5, 3: 0.625, 4: 0.8333333333333334, 5: 0.7142857142857143, 6: 0.5555555555555556} .",
            "url": "https://interkid.github.io/My_Research/2021/10/20/NetworkVisualization.html",
            "relUrl": "/2021/10/20/NetworkVisualization.html",
            "date": " â€¢ Oct 20, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "LSTM Algorithm & Implementation",
            "content": "Algorithm . Long-term-short-memory . . . TIme Series Analysis . purpose . Description | Modeling .. inference model and grasp feature time series | Prediction | Control .. manipulate variables to fluctuate purpose variable | TIme Series Analysis in RNN . RNN dose&#39;nt require prior knowledge for modeling trends &amp; period is learned from data autmatically | . | RNN weaksness when prediction of stock if there are only 3 years data, RNN can&#39;t perform well | . | . . Compare for methods in Time Series Forecasting . RNN(LSTM) is more expressionable in terms of function (but Blackbox) . Implementaion (pytorch) . Model Define . import torch import torch.nn as nn . input_dim = 5 # e.g. input of dimension 5 will look like this [1, 3, 8, 2, 3] hidden_dim = 10 # if the hidden dimension is 3 [3, 5, 4] n_layers = 1 # the number of LSTM layers stacked lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True) . Input Data . batch_size = 1 seq_len = 1 # initialize input, hidden state, cell state(long_memory) inp = torch.randn(batch_size, seq_len, input_dim) hidden_state = torch.randn(n_layers, batch_size, hidden_dim) cell_state = torch.randn(n_layers, batch_size, hidden_dim) hidden = (hidden_state, cell_state) print(&quot;inp_size: {}, n hidden_state_size: {}, n cell_state_size: {}&quot;.format(inp.shape, hidden_state.shape, cell_state.shape)) hidden . inp_size: torch.Size([1, 1, 5]), hidden_state_size: torch.Size([1, 1, 10]), cell_state_size: torch.Size([1, 1, 10]) . (tensor([[[ 2.7525, -0.3706, -0.1003, -0.8155, 0.1962, 1.3871, 0.4509, 0.3741, 0.4785, -0.6127]]]), tensor([[[ 1.1783, -0.6752, -1.6241, 0.0391, -1.1110, -0.5538, -1.5670, 0.1072, 0.0810, -0.4173]]])) . out, hidden = lstm_layer(inp, hidden) print(&quot;Outout shape:&quot;, out.shape) print(&quot;Hiden&quot;, hidden) . Outout shape: torch.Size([1, 1, 10]) Hiden (tensor([[[ 0.1804, -0.1010, -0.3767, -0.0170, -0.0909, 0.0929, -0.4508, 0.0109, 0.1320, -0.1107]]], grad_fn=&lt;StackBackward&gt;), tensor([[[ 0.2725, -0.1367, -0.8260, -0.0767, -0.1377, 0.2064, -1.0127, 0.0231, 0.2400, -0.1765]]], grad_fn=&lt;StackBackward&gt;)) . .. LSTM cell process the input and hidden states at each time step . seq_len = 3 inp = torch.randn(batch_size, seq_len, input_dim) out, hidden = lstm_layer(inp, hidden) print(out.shape) . torch.Size([1, 3, 10]) . .. the output&#39;s 2nd dimension is 3, indicating that there were 3 outputs given by the LSTM. This corresponds to the length of our input sequence. . Reference . viya-recurrent-neural-network.pdf | Long Short-Term Memory: From Zero to Hero with PyTorch | .",
            "url": "https://interkid.github.io/My_Research/2021/10/09/LSTM_introduction.html",
            "relUrl": "/2021/10/09/LSTM_introduction.html",
            "date": " â€¢ Oct 9, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "KNN Algorithm",
            "content": "KNN: What&#39;s it for . simple classification or regeression algorithm simple = low accuracy rather than other Algorithm | . | Regression Find the K closest points to a sample point and return the average value | . | . . The KNN&#8217;s steps are: . Receive an unclassified data; | Measure the distance (Euclidian, Manhattan, Minkowski or Weighted) from the new data to all others data that is already classified; | Gets the K(K is a parameter that you difine) smaller distances; | Check the list of classes had the shortest distance and count the amount of each class that appears; | Takes as correct class the class that appeared the most times; | Classifies the new data with the class that you took in step 5; | How is it used? . Dimensionality reduction stage Avoid sparse data | . | . Implementation Iris dataset . import numpy as np import matplotlib.pyplot as plt import pandas as pd . Data . url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot; names = [&#39;sepal-length&#39;, &#39;sepal-width&#39;, &#39;petal-length&#39;, &#39;petal-width&#39;, &#39;Class&#39;] dataset = pd.read_csv(url, names=names) . dataset . sepal-length sepal-width petal-length petal-width Class . 0 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | Iris-setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | Iris-setosa | . ... ... | ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | Iris-virginica | . 146 6.3 | 2.5 | 5.0 | 1.9 | Iris-virginica | . 147 6.5 | 3.0 | 5.2 | 2.0 | Iris-virginica | . 148 6.2 | 3.4 | 5.4 | 2.3 | Iris-virginica | . 149 5.9 | 3.0 | 5.1 | 1.8 | Iris-virginica | . 150 rows Ã— 5 columns . X = dataset.iloc[:, :-1].values y = dataset.iloc[:, 4].values # Train Test Split from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) # Feature Scaling from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) . KNN . from sklearn.neighbors import KNeighborsClassifier classifier = KNeighborsClassifier(n_neighbors=5) classifier.fit(X_train, y_train) . KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) . y_pred = classifier.predict(X_test) # Evaluating from sklearn.metrics import classification_report, confusion_matrix print(classification_report(y_test, y_pred)) . precision recall f1-score support Iris-setosa 1.00 1.00 1.00 8 Iris-versicolor 0.93 0.88 0.90 16 Iris-virginica 0.71 0.83 0.77 6 accuracy 0.90 30 macro avg 0.88 0.90 0.89 30 weighted avg 0.91 0.90 0.90 30 . K-means algorithm . K-means: What&#39;s it for . Unsupervised Learning for clustering | Non-hierarchical cluster / hierarchical cluster | . . The K-means steps are: . Select the value of K, to decide the number of clusters to be formed. | Select random K points which will act as centroids | Assign each data point, based on their distance from the randomly selected points (Centroid), to the nearest/closest centroid which will form the predefined clusters. | place a new centroid of each cluster. | Repeat step no.3 | If any reassignment occurs, then go to step-4 else go to Step 7. | Finish | Set randomly centeroid in step2, so It is important first value . to use effectively setting automatically initial centroid | . Reference . KNN (K-Nearest Neighbors) #1 | K Means Clustering Simplified in Python | K-Nearest Neighbors Algorithm in Python and Scikit-Learn | .",
            "url": "https://interkid.github.io/My_Research/2021/10/05/KNN.html",
            "relUrl": "/2021/10/05/KNN.html",
            "date": " â€¢ Oct 5, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://interkid.github.io/My_Research/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://interkid.github.io/My_Research/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . I am Kaito Honda software Deveoper in Japan.I am launching my career in data science. . I am lover of Science / Math. . . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://interkid.github.io/My_Research/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://interkid.github.io/My_Research/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}